{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "IR-project-attention-mechanism-demo.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prateekchandrajha/ir-mini-project/blob/main/IR_project_attention_mechanism_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3g-Gt-rUGBr"
      },
      "source": [
        "# The 3 Ways of Implementing Attention Mechanism and Dot Product Attention\n",
        "\n",
        "In this code walk-through we demonstrate cum explore the 3 ways of attention:\n",
        "- encoder-decoder attention, \n",
        "- causal attention, and\n",
        "- bi-directional self attention\n",
        "\n",
        "We'll also see how to implement the latter two with dot product attention. \n",
        "\n",
        "## Some Background\n",
        "\n",
        "As we saw in the Neural Machine Translation IR mini-project, **attention models** constitute powerful tools in the NLP and Information Retrieval practitioner's toolkit. Like LSTMs, they learn which words are most important to phrases, sentences, paragraphs, and so on. Moreover, they mitigate the vanishing gradient problem even better than LSTMs. We've already seen how to combine attention with LSTMs to build **encoder-decoder models** for applications such as machine translation tasks and word disambiguation tasks. \n",
        "\n",
        "<img src=\"https://github.com/prateekchandrajha/ir-mini-project/blob/main/attention_lnb_figs/C4_W2_L3_dot-product-attention_S01_introducing-attention_stripped.png?raw=1\" width=\"500\"/>\n",
        "\n",
        "Eventually we wanted to see how to integrate attention into **transformers** but we haven't been able to do the heavy-lifting required to build a transformer model just yet. We should understand that transformers are not sequence models, so they are much easier to parallelize and accelerate. Beyond machine translation, applications of transformers include: \n",
        "* Auto-completion\n",
        "* Named Entity Recognition\n",
        "* Chatbots\n",
        "* Question-Answering\n",
        "* And more!\n",
        "\n",
        "Along with embedding, positional encoding, dense layers, and residual connections, attention is a crucial component of transformers. At the heart of any attention scheme used in a transformer is **dot product attention**, of which the figures below display a simplified picture:\n",
        "\n",
        "<img src=\"https://github.com/prateekchandrajha/ir-mini-project/blob/main/attention_lnb_figs/C4_W2_L3_dot-product-attention_S03_concept-of-attention_stripped.png?raw=1\" width=\"500\"/>\n",
        "\n",
        "<img src=\"https://github.com/prateekchandrajha/ir-mini-project/blob/main/attention_lnb_figs/C4_W2_L3_dot-product-attention_S04_attention-math_stripped.png?raw=1\" width=\"500\"/>\n",
        "\n",
        "With basic dot product attention, you capture the interactions between every word (embedding) in your query and every word in your key. If the queries and keys belong to the same sentences, this constitutes **bi-directional self-attention**. In some situations, however, it's more appropriate to consider only words which have come before the current one. Such cases, particularly when the queries and keys come from the same sentences, fall into the category of **causal attention**. \n",
        "\n",
        "<img src=\"https://github.com/prateekchandrajha/ir-mini-project/blob/main/attention_lnb_figs/C4_W2_L4_causal-attention_S02_causal-attention_stripped.png?raw=1\" width=\"500\"/>\n",
        "\n",
        "For causal attention, we add a **mask** to the argument of our softmax function, as illustrated below: \n",
        "\n",
        "<img src=\"https://github.com/prateekchandrajha/ir-mini-project/blob/main/attention_lnb_figs/C4_W2_L4_causal-attention_S03_causal-attention-math_stripped.png?raw=1\" width=\"500\"/>\n",
        "\n",
        "<img src=\"https://github.com/prateekchandrajha/ir-mini-project/blob/main/attention_lnb_figs/C4_W2_L4_causal-attention_S04_causal-attention-math-2_stripped.png?raw=1\" width=\"500\"/>\n",
        "\n",
        "Now we'll see how to implement attention with NumPy. When we integrate attention into a transformer network defined with Trax, we shall have to use `trax.fastmath.numpy` instead, since Trax's arrays are based on JAX DeviceArrays. The function interfaces are often identical so not a lot of difference there."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07XNnXpIUGB1"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMREpzEOUGB1"
      },
      "source": [
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import scipy.special\n",
        "\n",
        "import textwrap\n",
        "wrapper = textwrap.TextWrapper(width=70)\n",
        "\n",
        "# to print the entire np array\n",
        "np.set_printoptions(threshold=sys.maxsize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ju5O5Ve3UGB2"
      },
      "source": [
        "Here are some helper functions that will help us create tensors and display useful information about those tensors when required:\n",
        "\n",
        "* `create_tensor()` creates a numpy array from a list of lists.\n",
        "* `display_tensor()` prints out the shape and the actual tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYrfl_7fUGB3"
      },
      "source": [
        "def create_tensor(t):\n",
        "    \"\"\"Create tensor from list of lists\"\"\"\n",
        "    return np.array(t)\n",
        "\n",
        "\n",
        "def display_tensor(t, name):\n",
        "    \"\"\"Display shape and tensor\"\"\"\n",
        "    print(f'{name} shape: {t.shape}\\n')\n",
        "    print(f'{t}\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZTt8qgYUGB3"
      },
      "source": [
        "Create some tensors and display their shapes. Please feel free to experiment with your own tensors. We should keep in mind, though, that the query, key, and value arrays must all have the same embedding dimensions (number of columns), and the mask array must have the same shape as `np.dot(query, key.T)`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcMyhz3ZUGB4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76f25cf8-0adc-45ce-b1c2-1d6431bfd797"
      },
      "source": [
        "q = create_tensor([[1, 0, 0], [0, 1, 0]])\n",
        "display_tensor(q, 'query')\n",
        "k = create_tensor([[1, 2, 3], [4, 5, 6]])\n",
        "display_tensor(k, 'key')\n",
        "v = create_tensor([[0, 1, 0], [1, 0, 1]])\n",
        "display_tensor(v, 'value')\n",
        "m = create_tensor([[0, 0], [-1e9, 0]])\n",
        "display_tensor(m, 'mask')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "query shape: (2, 3)\n",
            "\n",
            "[[1 0 0]\n",
            " [0 1 0]]\n",
            "\n",
            "key shape: (2, 3)\n",
            "\n",
            "[[1 2 3]\n",
            " [4 5 6]]\n",
            "\n",
            "value shape: (2, 3)\n",
            "\n",
            "[[0 1 0]\n",
            " [1 0 1]]\n",
            "\n",
            "mask shape: (2, 2)\n",
            "\n",
            "[[ 0.e+00  0.e+00]\n",
            " [-1.e+09  0.e+00]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udl7iIi5UGB5"
      },
      "source": [
        "## Dot product attention\n",
        "\n",
        "Here we come to the crux of this code walk-through, in which we compute \n",
        "$\\textrm{softmax} \\left(\\frac{Q K^T}{\\sqrt{d}} + M \\right) V$, where the scaling factor $\\sqrt{d}$ is the square root of the embedding dimension."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRzBZsZLUGB5"
      },
      "source": [
        "def DotProductAttention(query, key, value, mask, scale=True):\n",
        "    \"\"\"Dot product self-attention.\n",
        "    Args:\n",
        "        query (numpy.ndarray): array of query representations with shape (L_q by d)\n",
        "        key (numpy.ndarray): array of key representations with shape (L_k by d)\n",
        "        value (numpy.ndarray): array of value representations with shape (L_k by d) where L_v = L_k\n",
        "        mask (numpy.ndarray): attention-mask, gates attention with shape (L_q by L_k)\n",
        "        scale (bool): whether to scale the dot product of the query and transposed key\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Self-attention array for q, k, v arrays. (L_q by L_k)\n",
        "    \"\"\"\n",
        "\n",
        "    assert query.shape[-1] == key.shape[-1] == value.shape[-1], \"Checking whether the embedding dimensions of q, k, v aren't all the same\"\n",
        "\n",
        "    # Save depth/dimension of the query embedding for scaling down the dot product\n",
        "    if scale: \n",
        "        depth = query.shape[-1]\n",
        "    else:\n",
        "        depth = 1\n",
        "\n",
        "    # Calculate scaled query key dot product according to formula above\n",
        "    # Interchange two axes i.e. one axis from the last and second axis from the last of an array using np.swapaxes\n",
        "    dots = np.matmul(query, np.swapaxes(key, -1, -2)) / np.sqrt(depth) \n",
        "    \n",
        "    # Apply the mask\n",
        "    if mask is not None:\n",
        "        dots = np.where(mask, dots, np.full_like(dots, -1e9)) \n",
        "    \n",
        "    # Softmax formula implementation\n",
        "    # As we learnt we should use scipy.special.logsumexp of masked_qkT to avoid underflow by division by large numbers\n",
        "    # Remember: softmax = e^(dots - logaddexp(dots)) = E^dots / sumexp(dots)\n",
        "    logsumexp = scipy.special.logsumexp(dots, axis=-1, keepdims=True)\n",
        "\n",
        "    # Take exponential of dots minus logsumexp to get softmax\n",
        "    # We use np.exp()\n",
        "    dots = np.exp(dots - logsumexp)\n",
        "\n",
        "    # Matrix Multiply dots by value to get self-attention\n",
        "    # Use np.matmul()\n",
        "    attention = np.matmul(dots, value)\n",
        "    \n",
        "    return attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AX2fyXOZUGB6"
      },
      "source": [
        "Now let's implement the *masked* dot product self-attention (at the heart of **causal attention**) as a special case of dot product attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_b1kb4cyUGB6"
      },
      "source": [
        "def dot_product_self_attention(q, k, v, scale=True):\n",
        "    \"\"\" Masked dot product self attention.\n",
        "    Args:\n",
        "        q (numpy.ndarray): queries.\n",
        "        k (numpy.ndarray): keys.\n",
        "        v (numpy.ndarray): values.\n",
        "    Returns:\n",
        "        numpy.ndarray: masked dot product self attention tensor.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Size of the penultimate dimension of the query\n",
        "    mask_size = q.shape[-2]\n",
        "\n",
        "    # Creates a matrix with ones below the diagonal and 0s above. It should have shape (1, mask_size, mask_size)\n",
        "    # Use np.tril() - Lower triangle of an array and np.ones()\n",
        "    mask = np.tril(np.ones((1, mask_size, mask_size), dtype=np.bool_), k=0)  \n",
        "        \n",
        "    return DotProductAttention(q, k, v, mask, scale=scale)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dxx73CCZUGB6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55f4014b-3327-4e46-8805-13e8bde9ea66"
      },
      "source": [
        "dot_product_self_attention(q, k, v)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0.        , 1.        , 0.        ],\n",
              "        [0.84967455, 0.15032545, 0.84967455]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    }
  ]
}