{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "coursera": {
      "schema_names": [
        "NLPC4-1"
      ]
    },
    "jupytext": {
      "encoding": "# -*- coding: utf-8 -*-",
      "formats": "ipynb,py:percent"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "IR-translation-retrieval-attention-mechanism.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "XGc3cag6Iege",
        "5nhkQTlnIegf",
        "WBfowB3QIegg",
        "XgH-qijAIegj",
        "Xe7bfTFLIegl",
        "3Z_iyYN1Iegm",
        "kciKo3K9Iegm",
        "qdQ_SB6HIegn",
        "po7F5KwLIego",
        "SG5BEcqVIegq"
      ],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "32aa01f6af524fe2b7a069516ccae84a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d8b79074c6ff473b975b540d35363a54",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_935749500138465b80f5073a4be04791",
              "IPY_MODEL_0edb738b1002490a99b1b96d77c032f0"
            ]
          }
        },
        "d8b79074c6ff473b975b540d35363a54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "935749500138465b80f5073a4be04791": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_84fc8c9c2d784998aac80e9d4586e28a",
            "_dom_classes": [],
            "description": "Dl Completed...: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dce737d35f2e4c0dbc49915ac1f425cb"
          }
        },
        "0edb738b1002490a99b1b96d77c032f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ad2201408b9b4a6f84bfb851ed99d6ce",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1 [00:06&lt;00:00,  6.17s/ url]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3a9f6b45933543d9bd9eeff4cc207984"
          }
        },
        "84fc8c9c2d784998aac80e9d4586e28a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dce737d35f2e4c0dbc49915ac1f425cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ad2201408b9b4a6f84bfb851ed99d6ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3a9f6b45933543d9bd9eeff4cc207984": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f30601d8f88b4c1eb72ca45d8bc8e611": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_16850fb1517a4697a8ce8db06ef853ca",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3b57c89532674561b4e273a8ce332925",
              "IPY_MODEL_169bfa2e406e44e4bd9e1096188e34ba"
            ]
          }
        },
        "16850fb1517a4697a8ce8db06ef853ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3b57c89532674561b4e273a8ce332925": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8cfa3316741340f0872fb1c2ff99dd87",
            "_dom_classes": [],
            "description": "Dl Size...: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_99a87d6a8d2e4fe5a8b00af6cd4749aa"
          }
        },
        "169bfa2e406e44e4bd9e1096188e34ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b8f51b57a2e742e080de8dce4bea87c4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 34/34 [00:06&lt;00:00,  5.54 MiB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4dd4bc53e58e4c069347616567a837fe"
          }
        },
        "8cfa3316741340f0872fb1c2ff99dd87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "99a87d6a8d2e4fe5a8b00af6cd4749aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b8f51b57a2e742e080de8dce4bea87c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4dd4bc53e58e4c069347616567a837fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f58193b1bc68474ead8891371ac1dd24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1952cfe4b49c49c7a7eeff9cdd2ae3b8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b48f1c0a030742e3ae83155a92122b3c",
              "IPY_MODEL_782f483f8c9546b787871f46cb519119"
            ]
          }
        },
        "1952cfe4b49c49c7a7eeff9cdd2ae3b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b48f1c0a030742e3ae83155a92122b3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_106f0c2bb8fb4f02a8894cf9fdb7460d",
            "_dom_classes": [],
            "description": "Extraction completed...: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_87f40bb412684c6caecd7bc905dc629f"
          }
        },
        "782f483f8c9546b787871f46cb519119": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_834d7227222d4eb6b21265b0ff1359b8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1 [00:06&lt;00:00,  6.08s/ file]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_40128ab39e4646b79354c90db531f360"
          }
        },
        "106f0c2bb8fb4f02a8894cf9fdb7460d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "87f40bb412684c6caecd7bc905dc629f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "834d7227222d4eb6b21265b0ff1359b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "40128ab39e4646b79354c90db531f360": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fae5a07bbf5c42b1b27deca23365798a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_74a20a7a30e140969baa13c643891a11",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9fab542b08b745e098d8f52eaa026744",
              "IPY_MODEL_cfad08255ae84953a15e70e4f6c609d3"
            ]
          }
        },
        "74a20a7a30e140969baa13c643891a11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9fab542b08b745e098d8f52eaa026744": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_80a858b42f1d4e8bb0b7bf16342de25b",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d0d17f84d03047f192287fd941a2cb40"
          }
        },
        "cfad08255ae84953a15e70e4f6c609d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e12bddcff12d4dfb879e1393c5130fa3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1 [03:57&lt;00:00, 237.73s/ splits]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dfc363f617584b86a57cef8f5a6d1005"
          }
        },
        "80a858b42f1d4e8bb0b7bf16342de25b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d0d17f84d03047f192287fd941a2cb40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e12bddcff12d4dfb879e1393c5130fa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dfc363f617584b86a57cef8f5a6d1005": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "857f1435012741bf8a88636fa5cd25f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_93f69404c780424189ff25569126effb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e0b159f516b04faab85cea809ef2ff9d",
              "IPY_MODEL_4c867914b90a4eba8f2e4c2df8c40bdc"
            ]
          }
        },
        "93f69404c780424189ff25569126effb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e0b159f516b04faab85cea809ef2ff9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9a3826ae588d42f997b5b43781d99ac2",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1108752,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1108752,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_34429ddc3eea498c8af5a777c6b4676e"
          }
        },
        "4c867914b90a4eba8f2e4c2df8c40bdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_08d5cdab850945a8a72c823f1e356fcb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1108752/1108752 [03:52&lt;00:00, 4614.23 examples/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e7a4cdaa40964461bae2707363a9edac"
          }
        },
        "9a3826ae588d42f997b5b43781d99ac2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "34429ddc3eea498c8af5a777c6b4676e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "08d5cdab850945a8a72c823f1e356fcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e7a4cdaa40964461bae2707363a9edac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "845b8c19e2924367bd335d14172022dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8e5303c2cb574ba58eed38a3df606735",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f8d7e78ac74141ebb53c84037b65bf1f",
              "IPY_MODEL_93db46098cc64d398a5bb723b1b8284c"
            ]
          }
        },
        "8e5303c2cb574ba58eed38a3df606735": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f8d7e78ac74141ebb53c84037b65bf1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b63876a3b47142b097be3fb7554f9296",
            "_dom_classes": [],
            "description": " 97%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 1108752,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1076849,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_72aeefc4f6f84d839a7893714158e8ae"
          }
        },
        "93db46098cc64d398a5bb723b1b8284c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_849b47d39433468996300906bb00690b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1076849/1108752 [00:06&lt;00:03, 7995.98 examples/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2f19d13cf3a64170babdd636514dfd8f"
          }
        },
        "b63876a3b47142b097be3fb7554f9296": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "72aeefc4f6f84d839a7893714158e8ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "849b47d39433468996300906bb00690b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2f19d13cf3a64170babdd636514dfd8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prateekchandrajha/ir-mini-project/blob/main/IR_translation_retrieval_attention_mechanism.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPZkn0FDIegF"
      },
      "source": [
        "# Neural Machine Translation Using Attention Models (IR Mini-Project)\n",
        "\n",
        "As we highlighted in our project proposal, our target was to explore the mathematics behind Attention mechanism, causal attention, bidirectional attention and multi-headed attention employed by Google Search’s BERT models to explain the procedure of conducting a translation retrieval mechanism and text summarization mechanism. In this part of the submission, we implement a translation retrieval mechanism using Neural Network architectures and Attention mechanism.\n",
        "\n",
        "In the code below, we try and build an English-to-German neural machine translation (NMT) model using Long Short-Term Memory (LSTM) neural networks with attention mechanism. \n",
        "\n",
        "Machine translation is an important task in natural language processing and information retrieval at large. It could be useful not only for translating one language to another but also for word sense disambiguation (e.g. determining whether the word \"bank\" refers to the financial bank, or the land alongside a river). \n",
        "\n",
        "Implementing this using just a Recurrent Neural Network (RNN) with LSTMs can work for short to medium length sentences but can result in vanishing gradients (explained in the project report) for very long sequences. To solve this issue, we will be adding an attention mechanism to allow the decoder to access all relevant parts of the input sentence regardless of its length. \n",
        "\n",
        "There are many parts to this code. Let's segment what each part achieves:\n",
        "- preprocessing our training and evaluation data\n",
        "- implement an encoder-decoder system with attention\n",
        "- understand how attention mechanism works and subsequently implementing it as explained in the project report\n",
        "- build the Neural Machine Translation (NMT) model from scratch using Google Brain's Trax library\n",
        "- generate translations using Greedy and Minimum Bayes Risk (MBR) decoding \n",
        "\n",
        "## Outline/Contents Of Code\n",
        "- [Part 1:  Data Preparation](#1)\n",
        "    - [1.1  Importing the Data](#1.1)\n",
        "    - [1.2  Tokenization and Formatting](#1.2)\n",
        "    - [1.3  tokenize & detokenize helper functions](#1.3)\n",
        "    - [1.4  Bucketing](#1.4)\n",
        "    - [1.5  Exploring the data](#1.5)\n",
        "\n",
        "- [Part 2:  Neural Machine Translation with Attention](#2)\n",
        "    - [2.1  Attention Overview](#2.1)\n",
        "    - [2.2  Helper functions](#2.2)\n",
        "    - [2.3  Implementation Overview](#2.3)\n",
        "  \n",
        "- [Part 3:  Training](#3)\n",
        "    - [3.1  TrainTask](#3.1)\n",
        "    - [3.2  EvalTask](#3.2)\n",
        "    - [3.3  Loop](#3.3)\n",
        "\n",
        "- [Part 4:  Testing](#4)\n",
        "    - [4.1  Decoding](#4.1)\n",
        "    - [4.2  Minimum Bayes-Risk Decoding](#4.2)\n",
        "        \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJPaX6yDIegR"
      },
      "source": [
        "<a name=\"1\"></a>\n",
        "# Part 1:  Data Preparation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEZWtWL8IegS"
      },
      "source": [
        "<a name=\"1.1\"></a>\n",
        "## 1.1  Importing the Data\n",
        "\n",
        "We will first start by importing the packages we will be using to build our models. We will use the [Trax](https://github.com/google/trax) library created and maintained by the [Google Brain team](https://research.google/teams/brain/) to do most of the heavy lifting as far as creating the neural network architecture as well as the attention mechanism is concerned. It provides submodules to fetch and process the datasets, as well as build and train the model. This is precisely why we chose [Trax](https://github.com/google/trax) because we didn't want to get lost in the maze of Keras OR Pytorch OR Caffe. [Trax](https://github.com/google/trax) is SIMPLE, EFFECTIVE and GETS THE WORK DONE (ESPECIALLY THE DEEP LEARNING COMPONENT OF ONE'S WORK)!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFy1wOV3Mnap",
        "outputId": "e939a088-52f1-4c0c-f03a-c98a689bb70b"
      },
      "source": [
        "!pip install trax"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting trax\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/1d/c0a3aeed127c26a0c3f0925fc9cc7278c272e52310eedfc322477e854972/trax-1.3.6-py2.py3-none-any.whl (468kB)\n",
            "\u001b[K     |████████████████████████████████| 471kB 5.9MB/s \n",
            "\u001b[?25hCollecting tensorflow-text\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/b2/2dbd90b93913afd07e6101b8b84327c401c394e60141c1e98590038060b3/tensorflow_text-2.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6MB 18.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from trax) (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from trax) (1.4.1)\n",
            "Collecting funcsigs\n",
            "  Downloading https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.6/dist-packages (from trax) (0.1.57+cuda101)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from trax) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from trax) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from trax) (1.18.5)\n",
            "Collecting t5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/39/a607d2450190af7675e4f77c5eff0cc9a83f82fe63fb396872ef2004106b/t5-0.7.1-py3-none-any.whl (172kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 35.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.6/dist-packages (from trax) (4.0.1)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.6/dist-packages (from trax) (0.4.0)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.6/dist-packages (from trax) (0.2.7)\n",
            "Requirement already satisfied: tensorflow<2.4,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-text->trax) (2.3.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->trax) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->trax) (1.5.0)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.6/dist-packages (from jaxlib->trax) (1.12)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from t5->trax) (1.1.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from t5->trax) (3.2.5)\n",
            "Collecting rouge-score\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/56/a81022436c08b9405a5247b71635394d44fe7e1dbedc4b28c740e09c2840/rouge_score-0.0.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from t5->trax) (1.7.0+cu101)\n",
            "Requirement already satisfied: babel in /usr/local/lib/python3.6/dist-packages (from t5->trax) (2.9.0)\n",
            "Collecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/c4/8e948f601a4f9609e8b2b58f31966cb13cf17b940b82aa3e767f01c42c52/sacrebleu-1.4.14-py3-none-any.whl (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 6.2MB/s \n",
            "\u001b[?25hCollecting tfds-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/99/ce83f5cd8c74130ea557422bb0eb7d5dcdb9fb1e115a7748509fa2751fa7/tfds_nightly-4.1.0.dev202012120107-py3-none-any.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 40.4MB/s \n",
            "\u001b[?25hCollecting mesh-tensorflow[transformer]>=0.1.13\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/8b/553deb763ce8d00afb17debab7cb14a87b209cd4c6f0e8ecfc8d884cb12a/mesh_tensorflow-0.1.17-py3-none-any.whl (342kB)\n",
            "\u001b[K     |████████████████████████████████| 348kB 41.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from t5->trax) (0.22.2.post1)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 43.4MB/s \n",
            "\u001b[?25hCollecting transformers>=2.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/db/98c3ea1a78190dac41c0127a063abf92bd01b4b0b6970a6db1c2f5b66fa0/transformers-4.0.1-py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 42.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (0.3.3)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (0.25.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (3.12.4)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (0.1.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (4.41.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (2.3)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (3.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (0.16.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (0.8)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (1.1.0)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (20.3.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.6/dist-packages (from jax->trax) (3.3.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (0.3.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (0.2.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.34.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.12.1)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.1.2)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (2.10.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (2.3.0)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (2.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (0.36.1)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.6.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->t5->trax) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->t5->trax) (2.8.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->t5->trax) (3.7.4.3)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->t5->trax) (0.17.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5->trax) (20.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5->trax) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 43.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5->trax) (3.0.12)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 38.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow-datasets->trax) (1.52.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-datasets->trax) (50.3.2)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->tensorflow-datasets->trax) (3.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (3.0.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.7.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (3.3.3)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.17.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (0.4.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers>=2.7.0->t5->trax) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.7.0->t5->trax) (7.1.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (3.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (4.1.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (3.1.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=e22aea8e4fb7467a9fb99355e38fbb83a460cf1a913557ab58f8e59d80feda89\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tensorflow-text, funcsigs, rouge-score, portalocker, sacrebleu, tfds-nightly, mesh-tensorflow, sentencepiece, sacremoses, tokenizers, transformers, t5, trax\n",
            "Successfully installed funcsigs-1.0.2 mesh-tensorflow-0.1.17 portalocker-2.0.0 rouge-score-0.0.4 sacrebleu-1.4.14 sacremoses-0.0.43 sentencepiece-0.1.94 t5-0.7.1 tensorflow-text-2.3.0 tfds-nightly-4.1.0.dev202012120107 tokenizers-0.9.4 transformers-4.0.1 trax-1.3.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngkXbxPwIegT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c732385e-ab6e-47ac-cacb-5e967c556dfc"
      },
      "source": [
        "# We kept facing a number of warnings while working with Colab so we had to hide them using command below\n",
        "# It's not considered a good practice in general. Consult: https://stackoverflow.com/questions/9031783/hide-all-warnings-in-ipython\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from termcolor import colored\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import trax\n",
        "from trax import layers as tl\n",
        "from trax.fastmath import numpy as fastnp\n",
        "from trax.supervised import training\n",
        "\n",
        "!pip list | grep trax"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trax                          1.3.6                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPs_QIo1IegV"
      },
      "source": [
        "Next, we will import the dataset we will use to train the model. To meet the storage constraints in this Colab environment, we will just use a small dataset from [Opus](http://opus.nlpl.eu/), a growing collection of translated texts from the web. Particularly, we will get an English to German translation subset specified as `opus/medical` which has medical related texts. If storage is not an issue, you can opt to get a larger corpus such as the English to German translation dataset from [ParaCrawl](https://paracrawl.eu/), a large multi-lingual translation dataset created by the European Union. Both of these datasets are available via [Tensorflow Datasets (TFDS)](https://www.tensorflow.org/datasets)\n",
        "and we also browsed through the other available datasets [here](https://www.tensorflow.org/datasets/catalog/overview). \n",
        "\n",
        "We were unable to download the data into our [github project repository](https://github.com/prateekchandrajha/ir-mini-project) and unfortunately everytime you run this Colab notebook you have to download this dataset compulsorily to create data generators which we shall use to train our models. We'll download it from there. As one can find below, one can easily access this dataset from TFDS with `trax.data.TFDS`. \n",
        "\n",
        "The result is a python generator function yielding tuples. Use the `keys` argument to select what appears at which position in the tuple. For example, `keys=('en', 'de')` below will return pairs as (English sentence, German sentence). This makes our analysis very straight-forward."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eNlNU16IegV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296,
          "referenced_widgets": [
            "32aa01f6af524fe2b7a069516ccae84a",
            "d8b79074c6ff473b975b540d35363a54",
            "935749500138465b80f5073a4be04791",
            "0edb738b1002490a99b1b96d77c032f0",
            "84fc8c9c2d784998aac80e9d4586e28a",
            "dce737d35f2e4c0dbc49915ac1f425cb",
            "ad2201408b9b4a6f84bfb851ed99d6ce",
            "3a9f6b45933543d9bd9eeff4cc207984",
            "f30601d8f88b4c1eb72ca45d8bc8e611",
            "16850fb1517a4697a8ce8db06ef853ca",
            "3b57c89532674561b4e273a8ce332925",
            "169bfa2e406e44e4bd9e1096188e34ba",
            "8cfa3316741340f0872fb1c2ff99dd87",
            "99a87d6a8d2e4fe5a8b00af6cd4749aa",
            "b8f51b57a2e742e080de8dce4bea87c4",
            "4dd4bc53e58e4c069347616567a837fe",
            "f58193b1bc68474ead8891371ac1dd24",
            "1952cfe4b49c49c7a7eeff9cdd2ae3b8",
            "b48f1c0a030742e3ae83155a92122b3c",
            "782f483f8c9546b787871f46cb519119",
            "106f0c2bb8fb4f02a8894cf9fdb7460d",
            "87f40bb412684c6caecd7bc905dc629f",
            "834d7227222d4eb6b21265b0ff1359b8",
            "40128ab39e4646b79354c90db531f360",
            "fae5a07bbf5c42b1b27deca23365798a",
            "74a20a7a30e140969baa13c643891a11",
            "9fab542b08b745e098d8f52eaa026744",
            "cfad08255ae84953a15e70e4f6c609d3",
            "80a858b42f1d4e8bb0b7bf16342de25b",
            "d0d17f84d03047f192287fd941a2cb40",
            "e12bddcff12d4dfb879e1393c5130fa3",
            "dfc363f617584b86a57cef8f5a6d1005",
            "857f1435012741bf8a88636fa5cd25f3",
            "93f69404c780424189ff25569126effb",
            "e0b159f516b04faab85cea809ef2ff9d",
            "4c867914b90a4eba8f2e4c2df8c40bdc",
            "9a3826ae588d42f997b5b43781d99ac2",
            "34429ddc3eea498c8af5a777c6b4676e",
            "08d5cdab850945a8a72c823f1e356fcb",
            "e7a4cdaa40964461bae2707363a9edac",
            "845b8c19e2924367bd335d14172022dc",
            "8e5303c2cb574ba58eed38a3df606735",
            "f8d7e78ac74141ebb53c84037b65bf1f",
            "93db46098cc64d398a5bb723b1b8284c",
            "b63876a3b47142b097be3fb7554f9296",
            "72aeefc4f6f84d839a7893714158e8ae",
            "849b47d39433468996300906bb00690b",
            "2f19d13cf3a64170babdd636514dfd8f"
          ]
        },
        "outputId": "910d19de-0638-41c7-d4d7-b85a7e79c783"
      },
      "source": [
        "# Get generator function for the training set\n",
        "# This will download the train dataset if no data_dir is specified.\n",
        "train_stream_fn = trax.data.TFDS('opus/medical',\n",
        "                                 data_dir='./data/',\n",
        "                                 keys=('en', 'de'),\n",
        "                                 eval_holdout_size=0.01, # 1% holding out for evaluation purposes\n",
        "                                 train=True)\n",
        "\n",
        "# Get generator function for the eval set\n",
        "eval_stream_fn = trax.data.TFDS('opus/medical',\n",
        "                                data_dir='./data/',\n",
        "                                keys=('en', 'de'),\n",
        "                                eval_holdout_size=0.01, # 1% holding out for evaluation purposes\n",
        "                                train=False)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset opus/medical/0.1.0 (download: 34.29 MiB, generated: 188.85 MiB, total: 223.13 MiB) to ./data/opus/medical/0.1.0...\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "32aa01f6af524fe2b7a069516ccae84a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progre…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f30601d8f88b4c1eb72ca45d8bc8e611",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressSty…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f58193b1bc68474ead8891371ac1dd24",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Extraction completed...', max=1.0, styl…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fae5a07bbf5c42b1b27deca23365798a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "857f1435012741bf8a88636fa5cd25f3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1108752.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Shuffling and writing examples to data/opus/medical/0.1.0.incompleteYM9K4V/opus-train.tfrecord\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "845b8c19e2924367bd335d14172022dc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1108752.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\r\u001b[1mDataset opus downloaded and prepared to ./data/opus/medical/0.1.0. Subsequent calls will reuse this data.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-_4FpafIegW"
      },
      "source": [
        "Please notice that TFDS returns a generator *function*, not a python generator. This is because in Python, you cannot reset generators so you cannot go back to a previously yielded value. During deep learning training, you use Stochastic Gradient Descent and don't actually need to go back -- but it is sometimes good to be able to do that, and that's where the functions come in. \r\n",
        "\r\n",
        "It is actually very common to use generator functions in Python -- e.g., `zip` is a generator function. One can read more about [Python generators](https://book.pythontips.com/en/latest/generators.html) to understand why we use them. Let's print a a sample pair from our train and eval data to see what these data generators yield. Notice that the raw ouput is represented in bytes (denoted by the `b'` prefix) and these will be converted to strings internally in the next few coding steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VLV8jTqIegW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7744698a-d7b8-4e35-eb41-3243c7e74dd3"
      },
      "source": [
        "train_stream = train_stream_fn()\n",
        "print(colored('train data (en, de) tuple:', 'red'), next(train_stream))\n",
        "print()\n",
        "\n",
        "eval_stream = eval_stream_fn()\n",
        "print(colored('eval data (en, de) tuple:', 'red'), next(eval_stream))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mtrain data (en, de) tuple:\u001b[0m (b'Tel: +421 2 57 103 777\\n', b'Tel: +421 2 57 103 777\\n')\n",
            "\n",
            "\u001b[31meval data (en, de) tuple:\u001b[0m (b'Lutropin alfa Subcutaneous use.\\n', b'Pulver zur Injektion Lutropin alfa Subkutane Anwendung\\n')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGcDlVAGIegX"
      },
      "source": [
        "<a name=\"1.2\"></a>\n",
        "## 1.2  Tokenization and Formatting\n",
        "\n",
        "Now that we have imported our corpus, we will be preprocessing the sentences into a format that our model can accept. This will be composed of several steps:\n",
        "\n",
        "**TOKENIZING THE SENTENCES USING SUBWORD REPRESENTATIONS:** \n",
        "- As we can guess no model can understand words, so we want to represent each sentence as an array of integers instead of strings. For this specific application of ours, we will use *subword* representations to tokenize our sentences. \n",
        "- This is a common technique to avoid out-of-vocabulary words by allowing parts of words to be represented separately. For example, instead of having separate entries in your vocabulary for --\"fear\", \"fearless\", \"fearsome\", \"some\", and \"less\"--, you can simply store --\"fear\", \"some\", and \"less\"-- then allow your tokenizer to combine these subwords when needed. \n",
        "- This allows it to be more flexible so you won't have to save uncommon words explicitly in your vocabulary (e.g. *stylebender*, *nonce*, etc). Tokenizing is done with the `trax.data.Tokenize()` command and we download the combined subword vocabulary for English and German (i.e. `ende_32k.subword`) and save it in the `data` directory below. You can have a look at how the subwords look over here: [SUBWORDS](https://raw.githubusercontent.com/prateekchandrajha/ir-mini-project/main/data/ende_32k.subword)\n",
        "- The creator of Trax library says over [HERE](https://gitter.im/trax-ml/community?at=5f4838baec534f584fc6f401) that the ende_32k subword vocab was generated using Tensor2Tensor i.e. when you generate the WMT en-de data, it creates the vocabulary of subwords vocab. I think it's quite standard these days, e.g., it's used for [MLPerf](https://github.com/mlperf/training/tree/master/translation/tensorflow/transformer/vocab). I've not chosen to delve deeper into how exactly was this done."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SncASdgpShKD",
        "outputId": "c1338792-134d-4853-a2be-32ca7849cfb7"
      },
      "source": [
        "# !pip install wget\r\n",
        "import wget\r\n",
        "url = \"https://raw.githubusercontent.com/prateekchandrajha/ir-mini-project/main/data/ende_32k.subword\"\r\n",
        "wget.download(url)\r\n",
        "!ls"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " data  'ende_32k (1).subword'   ende_32k.subword   sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZybT771LWERa"
      },
      "source": [
        "!mv ende_32k.subword ./data/"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDWH0I4zIegX"
      },
      "source": [
        "# global variables that state the filename and directory of the vocabulary file\n",
        "VOCAB_FILE = 'ende_32k.subword'\n",
        "VOCAB_DIR = 'data/'\n",
        "\n",
        "# Tokenize the dataset.\n",
        "tokenized_train_stream = trax.data.Tokenize(vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)(train_stream)\n",
        "tokenized_eval_stream = trax.data.Tokenize(vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)(eval_stream)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9ItBuXoWYD-",
        "outputId": "7b1d21b5-c63a-4302-c60e-50e3765d65df"
      },
      "source": [
        "tokenized_train_stream"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object tokenize at 0x7fc600aff4c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvEWnj2BIegY"
      },
      "source": [
        "**APPEND AN END-OF-SENTENCE TOKEN TO EACH SENTENCE:** \r\n",
        "\r\n",
        "We will assign a token (i.e. in this case `1`) to mark the end of a sentence. This will definitely be useful in making inferences/predictions so we'll know that the model has completed the translation and has thereby stopped."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GZMXmHSIegY"
      },
      "source": [
        "# Append EOS at the end of each sentence.\n",
        "\n",
        "# Integer assigned as end-of-sentence (EOS)\n",
        "EOS = 1\n",
        "\n",
        "# generator helper function to append EOS to each sentence\n",
        "def append_eos(stream):\n",
        "    for (inputs, targets) in stream:\n",
        "        inputs_with_eos = list(inputs) + [EOS]\n",
        "        targets_with_eos = list(targets) + [EOS]\n",
        "        yield np.array(inputs_with_eos), np.array(targets_with_eos)\n",
        "\n",
        "# append EOS to the train data\n",
        "tokenized_train_stream = append_eos(tokenized_train_stream)\n",
        "\n",
        "# append EOS to the eval data\n",
        "tokenized_eval_stream = append_eos(tokenized_eval_stream)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4X3M1DCIegZ"
      },
      "source": [
        "**WE HAVE GOT TO FILTER LONG SENTENCES:** \r\n",
        "\r\n",
        "We will place a limit on the number of tokens per sentence to ensure we won't run out of memory. This is done with the `trax.data.FilterByLength()` method inside the `trax` library and you can see its syntax below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqLNYXawIegZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2bb5546-582e-4f85-fc41-5039ef4c784b"
      },
      "source": [
        "# Filter too long sentences to not run out of memory.\n",
        "# length_keys=[0, 1] means we filter both English and German sentences, so\n",
        "# both shouldn't be longer that 256 tokens for training / 512 for eval.\n",
        "\n",
        "filtered_train_stream = trax.data.FilterByLength(\n",
        "    max_length=256, length_keys=[0, 1])(tokenized_train_stream)\n",
        "    \n",
        "filtered_eval_stream = trax.data.FilterByLength(\n",
        "    max_length=512, length_keys=[0, 1])(tokenized_eval_stream)\n",
        "\n",
        "# print a sample input-target pair of tokenized sentences\n",
        "train_input, train_target = next(filtered_train_stream)\n",
        "print(colored(f'Single tokenized example input:', 'red' ), train_input)\n",
        "print(colored(f'Single tokenized example target:', 'red'), train_target)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mSingle tokenized example input:\u001b[0m [ 2538  2248    30 12114 23184 16889     5     2 20852  6456 20592  5812\n",
            "  3932    96  5178  3851    30  7891  3550 30650  4729   992     1]\n",
            "\u001b[31mSingle tokenized example target:\u001b[0m [ 1872    11  3544    39  7019 17877 30432    23  6845    10 14222    47\n",
            "  4004    18 21674     5 27467  9513   920   188 10630    18  3550 30650\n",
            "  4729   992     1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_Ij_9NFIega"
      },
      "source": [
        "<a name=\"1.3\"></a>\n",
        "## 1.3  tokenize & detokenize helper functions\n",
        "\n",
        "Given any data set, we should be able to map words to their indices, and indices to their words. The inputs and outputs to our trax models are usually tensors of numbers where each number corresponds to a word. If you were to process your data manually, you would have to make use of the following: \n",
        "\n",
        "- <span style='color:blue'> word2Ind: </span> a dictionary mapping the word to its index.\n",
        "- <span style='color:blue'> ind2Word:</span> a dictionary mapping the index to its word.\n",
        "- <span style='color:blue'> word2Count:</span> a dictionary mapping the word to the number of times it appears. \n",
        "- <span style='color:blue'> num_words:</span> total number of words that have appeared. \n",
        "\n",
        "The helper functions below does all the above for us:\n",
        "\n",
        "- <span style='color:blue'> tokenize(): </span> converts a text sentence to its corresponding token list (i.e. list of indices). Also converts words to subwords (parts of words).\n",
        "- <span style='color:blue'> detokenize(): </span> converts a token list to its corresponding sentence (i.e. string)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Tamrs8CIega"
      },
      "source": [
        "# Setup helper functions for tokenizing and detokenizing sentences\n",
        "\n",
        "def tokenize(input_str, vocab_file=None, vocab_dir=None):\n",
        "    \"\"\"Encodes a string to an array of integers\n",
        "\n",
        "    Args:\n",
        "        input_str (str): human-readable string to encode\n",
        "        vocab_file (str): filename of the vocabulary text file\n",
        "        vocab_dir (str): path to the vocabulary file\n",
        "  \n",
        "    Returns:\n",
        "        numpy.ndarray: tokenized version of the input string\n",
        "    \"\"\"\n",
        "    \n",
        "    # Set the encoding of the \"end of sentence\" as 1\n",
        "    EOS = 1\n",
        "    \n",
        "    # Use the trax.data.tokenize method. It takes streams and returns streams,\n",
        "    # we get around it by making a 1-element stream with `iter`.\n",
        "    inputs =  next(trax.data.tokenize(iter([input_str]),\n",
        "                                      vocab_file=vocab_file, vocab_dir=vocab_dir))\n",
        "    \n",
        "    # Mark the end of the sentence with EOS token\n",
        "    inputs = list(inputs) + [EOS]\n",
        "    \n",
        "    # Adding the batch dimension to the front of the shape\n",
        "    # [1,-1] makes sure it's a 1-dim numpy array, -1 makes sure numpy takes care of the dimensional setting\n",
        "    batch_inputs = np.reshape(np.array(inputs), [1, -1])\n",
        "    \n",
        "    return batch_inputs\n",
        "\n",
        "\n",
        "def detokenize(integers, vocab_file=None, vocab_dir=None):\n",
        "    \"\"\"Decodes an array of integers to a human readable string\n",
        "\n",
        "    Args:\n",
        "        integers (numpy.ndarray): array of integers to decode\n",
        "        vocab_file (str): filename of the vocabulary text file\n",
        "        vocab_dir (str): path to the vocabulary file\n",
        "  \n",
        "    Returns:\n",
        "        str: the decoded sentence.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Remove the dimensions of size 1\n",
        "    # Remove single-dimensional entries from the shape of a numpy array\n",
        "    integers = list(np.squeeze(integers))\n",
        "    \n",
        "    # Set the encoding of the \"end of sentence\" as 1\n",
        "    EOS = 1\n",
        "    \n",
        "    # Remove the EOS to decode only the original tokens\n",
        "    if EOS in integers:\n",
        "        integers = integers[:integers.index(EOS)] \n",
        "    \n",
        "    return trax.data.detokenize(integers, vocab_file=vocab_file, vocab_dir=vocab_dir)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQ2PxtTEIegb"
      },
      "source": [
        "Let's see how we might use these functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu-94m1iIegb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "193e626e-e57a-437d-bece-c475edd05a99"
      },
      "source": [
        "# As declared earlier:\n",
        "# VOCAB_FILE = 'ende_32k.subword'\n",
        "# VOCAB_DIR = 'data/'\n",
        "\n",
        "# Detokenize an input-target pair of tokenized sentences\n",
        "print(colored(f'Single detokenized example input:', 'red'), detokenize(train_input, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\n",
        "print(colored(f'Single detokenized example target:', 'red'), detokenize(train_target, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\n",
        "print()\n",
        "\n",
        "# Tokenize and detokenize a word that is not explicitly saved in the vocabulary file.\n",
        "# See how it combines the subwords -- 'hell' and 'o'-- to form the word 'hello'.\n",
        "print(colored(f\"tokenize('hello'): \", 'green'), tokenize('hello', vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\n",
        "print(colored(f\"detokenize([17332, 140, 1]): \", 'green'), detokenize([17332, 140, 1], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mSingle detokenized example input:\u001b[0m During treatment with olanzapine, adolescents gained significantly more weight compared with adults.\n",
            "\n",
            "\u001b[31mSingle detokenized example target:\u001b[0m Während der Behandlung mit Olanzapin nahmen die Jugendlichen im Vergleich zu Erwachsenen signifikant mehr Gewicht zu.\n",
            "\n",
            "\n",
            "\u001b[32mtokenize('hello'): \u001b[0m [[17332   140     1]]\n",
            "\u001b[32mdetokenize([17332, 140, 1]): \u001b[0m hello\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZW_UHJpIegc"
      },
      "source": [
        "<a name=\"1.4\"></a>\n",
        "## 1.4  Implementing Bucketing Technique\n",
        "\n",
        "Bucketing the tokenized sentences is an important technique used to speed up training in NLP. We found a [nice article describing it in detail](https://medium.com/@rashmi.margani/how-to-speed-up-the-training-of-the-sequence-model-using-bucketing-techniques-9e302b0fd976)\n",
        "but the gist is very simple. Our inputs have variable lengths and we want to make these the same when batching groups of sentences together. One way to do that is to pad each sentence to the length of the longest sentence in the dataset. This might lead to a lot of wasted computation though. \n",
        "\n",
        "For example, if there are multiple short sentences with just two tokens, do we want to pad these when the longest sentence is composed of a 100 tokens? Instead of padding with 0s to the maximum length of a sentence each time, we can group our tokenized sentences by length and bucket, as one can see in this image (from the article above):\n",
        "\n",
        "![alt text](https://miro.medium.com/max/700/1*hcGuja_d5Z_rFcgwe9dPow.png)\n",
        "\n",
        "We batch the sentences with similar length together (e.g. the blue sentences in the image above) and only add minimal padding to make them have equal length (usually up to the nearest power of two). This allows us to save a kot of wasted computation when processing padded sequences.\n",
        "\n",
        "In Trax, we found it implemented in the [bucket_by_length](https://github.com/google/trax/blob/5fb8aa8c5cb86dabb2338938c745996d5d87d996/trax/supervised/inputs.py#L378) function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojK4SBwGIegc"
      },
      "source": [
        "# Bucketing to create streams of batches.\n",
        "\n",
        "# Buckets are defined in terms of boundaries and batch sizes.\n",
        "# Batch_sizes[i] determines the batch size for items with length < boundaries[i]\n",
        "# So below, we'll take a batch of 256 sentences of length < 8, 128 if length is\n",
        "# between 8 and 16, and so on -- and only 2 if length is over 512.\n",
        "\n",
        "boundaries =  [8,   16,  32, 64, 128, 256, 512]\n",
        "batch_sizes = [256, 128, 64, 32, 16,    8,   4,  2]\n",
        "\n",
        "# Create the generators.\n",
        "train_batch_stream = trax.data.BucketByLength(\n",
        "    boundaries, batch_sizes,\n",
        "    length_keys=[0, 1]  # As before: count inputs and targets to length.\n",
        ")(filtered_train_stream)\n",
        "\n",
        "eval_batch_stream = trax.data.BucketByLength(\n",
        "    boundaries, batch_sizes,\n",
        "    length_keys=[0, 1]  # As before: count inputs and targets to length.\n",
        ")(filtered_eval_stream)\n",
        "\n",
        "# Add masking for the padding (0s).\n",
        "train_batch_stream = trax.data.AddLossWeights(id_to_mask=0)(train_batch_stream)\n",
        "eval_batch_stream = trax.data.AddLossWeights(id_to_mask=0)(eval_batch_stream)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrfHf1_vIegc"
      },
      "source": [
        "<a name=\"1.5\"></a>\n",
        "## 1.5  Data Exploration Step\n",
        "\n",
        "We will now be displaying some of our data. One can see that the functions defined above (i.e. `tokenize()` and `detokenize()`) serve a very important role. Now we can focus more on building the translation retrieval model from scratch. Let us see whether our data generator works as we expect it to and get one batch of the data for testing purposes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2iUXto-Iegc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4d39cbf-4136-428a-e3c4-5afafb598f6d"
      },
      "source": [
        "input_batch, target_batch, mask_batch = next(train_batch_stream)\n",
        "\n",
        "# let's see the data type of a batch\n",
        "print(\"input_batch data type: \", type(input_batch))\n",
        "print(\"target_batch data type: \", type(target_batch))\n",
        "\n",
        "# let's see the shape of this particular batch (batch length, sentence length)\n",
        "print(\"input_batch shape: \", input_batch.shape)\n",
        "print(\"target_batch shape: \", target_batch.shape)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_batch data type:  <class 'numpy.ndarray'>\n",
            "target_batch data type:  <class 'numpy.ndarray'>\n",
            "input_batch shape:  (32, 64)\n",
            "target_batch shape:  (32, 64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRbOUOndIegc"
      },
      "source": [
        "The `input_batch` and `target_batch` are Numpy arrays consisting of tokenized English sentences and German sentences respectively. These tokens will later be used to produce embedding vectors for each word in the sentence (so the embedding for a sentence will be a matrix). As we saw online, practitioners suggest us to keep the number of sentences in each batch somewhat around a power of 2 for optimal computer memory usage. \n",
        "\n",
        "We can now visually inspect some of the data. One can run the cell below multiple times to shuffle through the sentences (because of randomness). Let's pick a random sentence and print its tokenized representation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkX1s4nJIegd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf70139a-e879-4ed2-f3c8-07d891b9f53b"
      },
      "source": [
        "# pick a random index less than the batch size.\n",
        "index = random.randrange(len(input_batch))\n",
        "\n",
        "# use the index to grab an entry from the input and target batch\n",
        "print(colored('THIS IS THE ENGLISH SENTENCE: \\n', 'red'), detokenize(input_batch[index], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR), '\\n')\n",
        "print(colored('THIS IS THE TOKENIZED VERSION OF THE ENGLISH SENTENCE: \\n ', 'red'), input_batch[index], '\\n')\n",
        "print(colored('THIS IS THE GERMAN TRANSLATION: \\n', 'red'), detokenize(target_batch[index], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR), '\\n')\n",
        "print(colored('THIS IS THE TOKENIZED VERSION OF THE GERMAN TRANSLATION: \\n', 'red'), target_batch[index], '\\n')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mTHIS IS THE ENGLISH SENTENCE: \n",
            "\u001b[0m In the pregnant rat the AUC for calculated free drug at this dose was approximately 18 times the human AUC at a 20 mg dose.\n",
            " \n",
            "\n",
            "\u001b[31mTHIS IS THE TOKENIZED VERSION OF THE ENGLISH SENTENCE: \n",
            " \u001b[0m [   71     4  3678 17363  8195     4  9227   469    19 20605   360  5575\n",
            "    68    49 20441    53  7408  1004  1195     4   433  9227   469    68\n",
            "    13   384 23306     5 20441  3550 30650  4729   992     1     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0] \n",
            "\n",
            "\u001b[31mTHIS IS THE GERMAN TRANSLATION: \n",
            "\u001b[0m Bei trächtigen Ratten war die AUC für die berechnete ungebundene Substanz bei dieser Dosis etwa 18-mal höher als die AUC beim Menschen bei einer 20 mg Dosis.\n",
            " \n",
            "\n",
            "\u001b[31mTHIS IS THE TOKENIZED VERSION OF THE GERMAN TRANSLATION: \n",
            "\u001b[0m [  752 22482 13831 15849   177   142    10  9227   469    25    10  8980\n",
            " 24481    35  4064 20618  4290 18098     5   113   143 14327    16   780\n",
            "  1004    15  2127  3695    69    10  9227   469   683   296   113    88\n",
            "   384 23306     5 14327    16  3550 30650  4729   992     1     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyUow88GIegd"
      },
      "source": [
        "<a name=\"2\"></a>\n",
        "# Part 2:  Neural Machine Translation with Attention\n",
        "\n",
        "Now that we have prepared the data generators and have handled the complete preprocessing required, it is time for us to build the actual neural translation retrieval model. We will be implementing a neural machine translation model from scratch with attention mechanism. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1K-p3qAIegd"
      },
      "source": [
        "<a name=\"2.1\"></a>\n",
        "## 2.1  Let's Revisit Attention Mechanism and Understand It Thoroughly\n",
        "\n",
        "The model we will be building uses an encoder-decoder architecture. This Recurrent Neural Network (RNN) will take in a tokenized version of a sentence in its encoder, then passes it on to the decoder for translation. \n",
        "\n",
        "As mentioned in the project report, just using a regular sequence-to-sequence (seq2seq) model with LSTMs will work effectively for short to medium sentences but will start to degrade for longer sentences. You can picture it like the figure below where all of the context of the input sentence is compressed into one vector that is passed into the decoder block. You can see how this will be an issue for very long sentences (e.g. 100 tokens or more) because the context of the first parts of the input will have very little effect on the final vector passed to the decoder.\n",
        "\n",
        "<img src='https://github.com/prateekchandrajha/ir-mini-project/blob/main/plain_rnn.png?raw=1'>\n",
        "\n",
        "As we explained in the project report, an attention layer to this model avoids this problem by giving the decoder access to all parts of the input sentence. To illustrate, let's just use a 4-word input sentence as shown below. Remember that a hidden state is produced at each timestep of the encoder (represented by the orange rectangles). These are all passed to the attention layer and each are given a score given the current activation (i.e. hidden state) of the decoder. For instance, let's consider the figure below where the first prediction \"Wie\" is already made. To produce the next prediction, the attention layer will first receive all the encoder hidden states (i.e. orange rectangles) as well as the decoder hidden state when producing the word \"Wie\" (i.e. first green rectangle). Given these information, it will score each of the encoder hidden states to know which one the decoder should focus on to produce the next word. The result of the model training might have learned that it should align to the second encoder hidden state and subsequently assigns a high probability to the word \"geht\". If we are using greedy decoding, we will output the said word as the next symbol, then restart the process to produce the next word until we reach an end-of-sentence prediction.\n",
        "\n",
        "<img src='https://github.com/prateekchandrajha/ir-mini-project/blob/main/attention_overview.png?raw=1'>\n",
        "\n",
        "\n",
        "There are different ways to implement attention and the one we'll use for this project is the Scaled Dot Product Attention which has the form:\n",
        "\n",
        "$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
        "\n",
        "We have delved deeper into this equation in the project report but for simplicity one can think of it as computing scores using queries (Q) and keys (K), followed by a multiplication of values (V) to get a context vector at a particular timestep of the decoder. This context vector is fed to the decoder RNN to get a set of probabilities for the next predicted word. The division by square root of the keys dimensionality ($\\sqrt{d_k}$) is for improving model performance (please consult the report for explanations regarding why is it so). For our machine translation application, the encoder activations (i.e. encoder hidden states) will be the keys and values, while the decoder activations (i.e. decoder hidden states) will be the queries.\n",
        "\n",
        "As we will see in the upcoming code walk-throughs that this complex neural architecture and attention mechanism can be implemented with just a few lines of code. This also justifies our usage of `trax` framework for completing the deep learning part of this project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btWzlixIIegd"
      },
      "source": [
        "<a name=\"2.2\"></a>\n",
        "## 2.2  Let's Code Some Helper functions\n",
        "\n",
        "We will first implement a few functions that we will use later on. These will be for the input encoder, pre-attention decoder, and preparation of the queries, keys, values, and mask.\n",
        "\n",
        "### 2.2.1 Input encoder\n",
        "\n",
        "The input encoder runs on the input tokens, creates its embeddings, and feeds it to an LSTM network. This outputs the activations that will be the keys and values for attention. It is a [Serial](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Serial) network which uses:\n",
        "\n",
        "   - [tl.Embedding](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding): Converts each token to its vector representation. In this case, it is the the size of the vocabulary by the dimension of the model: `tl.Embedding(vocab_size, d_model)`. `vocab_size` is the number of entries in the given vocabulary. `d_model` is the number of elements in the word embedding.\n",
        "  \n",
        "   - [tl.LSTM](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.rnn.LSTM): LSTM layer of size `d_model`. We want to be able to configure how many encoder layers we have so remember to create LSTM layers equal to the number of the `n_encoder_layers` parameter.\n",
        "   \n",
        "<img src = \"https://github.com/prateekchandrajha/ir-mini-project/blob/main/input_encoder.png?raw=1\">\n",
        "\n",
        "\n",
        "\n",
        "**INPUT ENCODER IMPLEMENTATION:** Below we Implement the `input_encoder_fn` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfoHjRVfIege"
      },
      "source": [
        "def input_encoder_fn(input_vocab_size, d_model, n_encoder_layers):\n",
        "    \"\"\" Input encoder runs on the input sentence and creates\n",
        "    activations that will be the keys and values for attention.\n",
        "    \n",
        "    Args:\n",
        "        input_vocab_size: int: vocab size of the input\n",
        "        d_model: int:  depth of embedding (n_units in the LSTM cell)\n",
        "        n_encoder_layers: int: number of LSTM layers in the encoder\n",
        "    Returns:\n",
        "        tl.Serial: The input encoder\n",
        "    \"\"\"\n",
        "    \n",
        "    # create a serial network\n",
        "    input_encoder = tl.Serial( \n",
        "        \n",
        "        # create an embedding layer to convert tokens to vectors\n",
        "        tl.Embedding(input_vocab_size, d_model),\n",
        "        \n",
        "        # feed the embeddings to the LSTM layers. It is a stack of n_encoder_layers LSTM layers\n",
        "        [tl.LSTM(d_model) for _ in range(n_encoder_layers)],\n",
        "        \n",
        "    )\n",
        "\n",
        "    return input_encoder"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGc3cag6Iege"
      },
      "source": [
        "### 2.2.2 Pre-attention decoder\n",
        "\n",
        "The pre-attention decoder runs on the targets and creates activations that are used as queries in attention. This is a Serial network which is composed of the following:\n",
        "\n",
        "   - [tl.ShiftRight](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.ShiftRight): This pads a token to the beginning of your target tokens (e.g. `[8, 34, 12]` shifted right is `[0, 8, 34, 12]`). This will act like a start-of-sentence token that will be the first input to the decoder. During training, this shift also allows the target tokens to be passed as input to do teacher forcing. Please consult [this excellent article to understand why Teacher Forcing](https://towardsdatascience.com/what-is-teacher-forcing-3da6217fed1c) is essential in language models training.\n",
        "\n",
        "   - [tl.Embedding](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding): Like in the previous function, this converts each token to its vector representation. In this case, it is the the size of the vocabulary by the dimension of the model: `tl.Embedding(vocab_size, d_model)`. `vocab_size` is the number of entries in the given vocabulary. `d_model` is the number of elements in the word embedding.\n",
        "   \n",
        "   - [tl.LSTM](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.rnn.LSTM): LSTM layer of size `d_model`.\n",
        "\n",
        "<img src = \"https://github.com/prateekchandrajha/ir-mini-project/blob/main/pre_attention_decoder.png?raw=1\">\n",
        "\n",
        "\n",
        "\n",
        "**PRE-ATTENTION DECODER IMPLEMENTATION:** Below we implement the `pre_attention_decoder_fn` function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAGlPUx2Iege"
      },
      "source": [
        "def pre_attention_decoder_fn(mode, target_vocab_size, d_model):\n",
        "    \"\"\" Pre-attention decoder runs on the targets and creates\n",
        "    activations that are used as queries in attention.\n",
        "    \n",
        "    Args:\n",
        "        mode: str: 'train' or 'eval'\n",
        "        target_vocab_size: int: vocab size of the target\n",
        "        d_model: int:  depth of embedding (n_units in the LSTM cell)\n",
        "    Returns:\n",
        "        tl.Serial: The pre-attention decoder\n",
        "    \"\"\"\n",
        "    \n",
        "    # create a serial network\n",
        "    pre_attention_decoder = tl.Serial(\n",
        "        \n",
        "        # shift right to insert start-of-sentence token and implement\n",
        "        # teacher forcing during training\n",
        "        tl.ShiftRight(mode=mode),\n",
        "\n",
        "        # run an embedding layer to convert tokens to vectors\n",
        "        tl.Embedding(target_vocab_size, d_model),\n",
        "\n",
        "        # feed to an LSTM layer\n",
        "        tl.LSTM(d_model)\n",
        "    )\n",
        "    \n",
        "    return pre_attention_decoder"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nhkQTlnIegf"
      },
      "source": [
        "### 2.2.3 Preparing the attention input\n",
        "\n",
        "This function will prepare the inputs to the attention layer. We want to take in the encoder and pre-attention decoder activations and assign it to the queries, keys, and values. In addition, another output here will be the mask to distinguish real tokens from padding tokens. This mask will be used internally by Trax when computing the softmax so padding tokens will not have an effect on the computated probabilities. From the data preparation steps we undertook in the beginning of this code, we should already know which tokens in the input correspond to padding and which don't.\n",
        "\n",
        "The last two lines of code in composing the mask includes a concept that we have described in our project report. This is related to *multiheaded attention* which one can simply think of right now as computing the attention multiple times over to improve the model's predictions. It is required to consider this additional axis in the output. What's important now is for us to know which should be our queries, keys, and values, as well as to initialize the mask.\n",
        "\n",
        "**ATTENTION INPUT PREPARATION:** Below we implement the  `prepare_attention_input` function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgWvf4B_Iegf"
      },
      "source": [
        "def prepare_attention_input(encoder_activations, decoder_activations, inputs):\n",
        "    \"\"\"Prepare queries, keys, values and mask for attention.\n",
        "    \n",
        "    Args:\n",
        "        encoder_activations fastnp.array(batch_size, padded_input_length, d_model): output from the input encoder\n",
        "        decoder_activations fastnp.array(batch_size, padded_input_length, d_model): output from the pre-attention decoder\n",
        "        inputs fastnp.array(batch_size, padded_input_length): padded input tokens\n",
        "    \n",
        "    Returns:\n",
        "        queries, keys, values and mask for attention.\n",
        "    \"\"\"\n",
        "    \n",
        "    # set the keys and values to the encoder activations\n",
        "    keys = encoder_activations\n",
        "    values = encoder_activations\n",
        "    \n",
        "    # set the queries to the decoder activations\n",
        "    queries = decoder_activations\n",
        "    \n",
        "    # generate the mask to distinguish real tokens from padding\n",
        "    # inputs is 1 for real tokens and 0 where they are padding hence inputs != 0 gives us the mask locations\n",
        "    mask = (inputs != 0)\n",
        "    \n",
        "    # add axes to the mask for attention heads and decoder length.\n",
        "    mask = fastnp.reshape(mask, (mask.shape[0], 1, 1, mask.shape[1]))\n",
        "    \n",
        "    # broadcast it so mask shape is [batch size, attention heads, decoder-len, encoder-len].\n",
        "    # for our project, attention heads is set to 1. No multi-headed attention because that could get overly complicated.\n",
        "    mask = mask + fastnp.zeros((1, 1, decoder_activations.shape[1], 1))\n",
        "        \n",
        "    \n",
        "    return queries, keys, values, mask"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaelbsEpIegg"
      },
      "source": [
        "<a name=\"2.3\"></a>\n",
        "## 2.3  Implementation Overview\n",
        "\n",
        "We are now ready to implement our sequence-to-sequence model with attention. This will be a Serial network and is illustrated in the diagram below. It shows us all the layers we shall be using in Trax and we shall see that each step can be implemented quite easily with one line commands. THIS AGAIN JUSTIFIES OUR USAGE OF `TRAX` LIBRARY. Documentation links for each relevant `trax` layer is attached for ready reference.\n",
        "\n",
        "<img src = \"https://github.com/prateekchandrajha/ir-mini-project/blob/main/NMTModel.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBfowB3QIegg"
      },
      "source": [
        "\n",
        "**NEURAL MACHINE TRANSLATION ATTENTION MECHANISM IMPLEMENTATION:** Below we implement the `NMTAttn` function to define our machine translation model which uses attention. \n",
        "\n",
        "**Step 0:** Firstly we prepare the input encoder and pre-attention decoder branches. We have already defined this earlier as helper functions so it's just a matter of calling those functions and assigning it to variables.\n",
        "\n",
        "**Step 1:** Next we create a Serial network. This will stack the layers in the next steps one after the other. Like we have done earlier, we will use [tl.Serial](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Serial).\n",
        "\n",
        "**Step 2:** Now we make a copy of the input and target tokens. As you can see in the diagram above, the input and target tokens will be fed into different layers of the model. We will use [tl.Select](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Select) layer to create copies of these tokens. Arrange them in the following order as `[input tokens, target tokens, input tokens, target tokens]`.\n",
        "\n",
        "**Step 3:** Now we need to create a parallel branch to feed the input tokens to the `input_encoder` and the target tokens to the `pre_attention_decoder`. To achieve this we will use [tl.Parallel](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Parallel) so as to create these sublayers in parallel. Here we pass the variables we defined in Step 0 as parameters to this layer.\n",
        "\n",
        "**Step 4:** Next, we call the `prepare_attention_input` function to convert the encoder and pre-attention decoder activations to a format that the attention layer will accept. We will use [tl.Fn](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn) to call this function. We pass the `prepare_attention_input` function as the `f` parameter in `tl.Fn` without any arguments or parenthesis.\n",
        "\n",
        "**Step 5:** Finally we will feed the (queries, keys, values, and mask) to the [tl.AttentionQKV](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.AttentionQKV) layer. This computes the **scaled dot product attention** and outputs the attention weights and mask. We should make it clear here that although this implementation is a one liner, this layer is actually composed of a deep network made up of several branches. We'll show the implementation taken from [here](https://github.com/google/trax/blob/master/trax/layers/attention.py#L61) to see the different layers used. Please go through this code below for clarity.\n",
        "\n",
        "```python\n",
        "def AttentionQKV(d_feature, n_heads=1, dropout=0.0, mode='train'):\n",
        "  \"\"\"Returns a layer that maps (q, k, v, mask) to (activations, mask).\n",
        "\n",
        "  See `Attention` above for further context/details.\n",
        "\n",
        "  Args:\n",
        "    d_feature: Depth/dimensionality of feature embedding.\n",
        "    n_heads: Number of attention heads.\n",
        "    dropout: Probababilistic rate for internal dropout applied to attention\n",
        "        activations (based on query-key pairs) before dotting them with values.\n",
        "    mode: Either 'train' or 'eval'.\n",
        "  \"\"\"\n",
        "  return cb.Serial(\n",
        "      cb.Parallel(\n",
        "          core.Dense(d_feature),\n",
        "          core.Dense(d_feature),\n",
        "          core.Dense(d_feature),\n",
        "      ),\n",
        "      PureAttention(  # pylint: disable=no-value-for-parameter\n",
        "          n_heads=n_heads, dropout=dropout, mode=mode),\n",
        "      core.Dense(d_feature),\n",
        "  )\n",
        "```\n",
        "\n",
        "Having deep layers pose the risk of vanishing gradients during training and we would want to mitigate that. To improve the ability of the network to learn, we insert a [tl.Residual](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Residual) layer to add the output of AttentionQKV with the `queries` input. We can do this in trax by simply nesting the `AttentionQKV` layer inside the `Residual` layer. The `trax` library will take care of branching and adding for us.\n",
        "\n",
        "**Step 6:** We will not need the mask for the model we're building so we can safely drop it. At this point in the network, the signal stack currently has `[attention activations, mask, target tokens]` and we can use [tl.Select](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Select) to output just `[attention activations, target tokens]`.\n",
        "\n",
        "**Step 7:** We can now feed the attention weighted output to the LSTM decoder. We can stack multiple [tl.LSTM](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.rnn.LSTM) layers to improve the output so we append LSTMs equal to the number defined by `n_decoder_layers` parameter to the model.\n",
        "\n",
        "**Step 8:** We want to determine the probabilities of each subword in the vocabulary and we set this up easily with a [tl.Dense](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense) layer by making its size equal to the size of our vocabulary.\n",
        "\n",
        "**Step 9:** The final step is to normalize the output to log probabilities by passing the activations in Step 8 to a [tl.LogSoftmax](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.LogSoftmax) layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rv4ZJ8wnIegg"
      },
      "source": [
        "def NMTAttn(input_vocab_size=33300,\n",
        "            target_vocab_size=33300,\n",
        "            d_model=1024,\n",
        "            n_encoder_layers=2,\n",
        "            n_decoder_layers=2,\n",
        "            n_attention_heads=4,\n",
        "            attention_dropout=0.0,\n",
        "            mode='train'):\n",
        "    \"\"\"Returns an LSTM sequence-to-sequence model with attention.\n",
        "\n",
        "    The input to the model is a pair (input tokens, target tokens), e.g.,\n",
        "    an English sentence (tokenized) and its translation into German (tokenized).\n",
        "\n",
        "    Args:\n",
        "    input_vocab_size: int: vocab size of the input\n",
        "    target_vocab_size: int: vocab size of the target\n",
        "    d_model: int:  depth of embedding (n_units in the LSTM cell)\n",
        "    n_encoder_layers: int: number of LSTM layers in the encoder\n",
        "    n_decoder_layers: int: number of LSTM layers in the decoder after attention\n",
        "    n_attention_heads: int: number of attention heads\n",
        "    attention_dropout: float, dropout for the attention layer\n",
        "    mode: str: 'train', 'eval' or 'predict', predict mode is for fast inference\n",
        "\n",
        "    Returns:\n",
        "    A LSTM sequence-to-sequence model with attention.\n",
        "    \"\"\"\n",
        "\n",
        "    # We expect the reader to have read the above steps before jumping right in. It helps in getting the gist of what's happening below.\n",
        "    # As you can see, most of the implementations below are a one liner in `trax` library\n",
        "    \n",
        "    # Step 0: call the helper function to create layers for the input encoder\n",
        "    input_encoder = input_encoder_fn(input_vocab_size, d_model, n_encoder_layers)\n",
        "\n",
        "    # Step 0: call the helper function to create layers for the pre-attention decoder\n",
        "    pre_attention_decoder = pre_attention_decoder_fn(mode, target_vocab_size, d_model)\n",
        "\n",
        "    # Step 1: create a serial network\n",
        "    model = tl.Serial( \n",
        "        \n",
        "      # Step 2: copy input tokens and target tokens as they will be needed later.\n",
        "      tl.Select([0, 1, 0, 1]),\n",
        "        \n",
        "      # Step 3: run input encoder on the input and pre-attention decoder the target.\n",
        "      tl.Parallel(input_encoder, pre_attention_decoder),\n",
        "        \n",
        "      # Step 4: prepare queries, keys, values and mask for attention.\n",
        "      tl.Fn('PrepareAttentionInput', prepare_attention_input, n_out=4),\n",
        "        \n",
        "      # Step 5: run the AttentionQKV layer\n",
        "      # nest it inside a Residual layer to add to the pre-attention decoder activations(i.e. queries)\n",
        "      # please read explanations above to understand why we used a residual layer over here\n",
        "      tl.Residual(tl.AttentionQKV(d_model, n_heads=n_attention_heads, dropout=attention_dropout, mode=mode)),\n",
        "      \n",
        "      # Step 6: drop attention mask (i.e. index = None\n",
        "      tl.Select([0, 2]),\n",
        "        \n",
        "      # Step 7: run the rest of the RNN decoder\n",
        "      [tl.LSTM(d_model) for _ in range(n_decoder_layers)],\n",
        "        \n",
        "      # Step 8: prepare output by making it the right size\n",
        "      tl.Dense(target_vocab_size),\n",
        "        \n",
        "      # Step 9: Log-softmax for output\n",
        "      tl.LogSoftmax()\n",
        "    )\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3YltZCaIegg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc799969-7d2f-4bd9-baac-d7eb8604a6ab"
      },
      "source": [
        "# printing our NMT model below\n",
        "model = NMTAttn()\n",
        "print(model)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Serial_in2_out2[\n",
            "  Select[0,1,0,1]_in2_out4\n",
            "  Parallel_in2_out2[\n",
            "    Serial[\n",
            "      Embedding_33300_1024\n",
            "      LSTM_1024\n",
            "      LSTM_1024\n",
            "    ]\n",
            "    Serial[\n",
            "      Serial[\n",
            "        AssertShape\n",
            "        ShiftRight(1)\n",
            "        AssertShape\n",
            "      ]\n",
            "      Embedding_33300_1024\n",
            "      LSTM_1024\n",
            "    ]\n",
            "  ]\n",
            "  PrepareAttentionInput_in3_out4\n",
            "  Serial_in4_out2[\n",
            "    Branch_in4_out3[\n",
            "      None\n",
            "      Serial_in4_out2[\n",
            "        AssertShape_in4_out4\n",
            "        Serial_in4_out2[\n",
            "          Parallel_in3_out3[\n",
            "            Dense_1024\n",
            "            Dense_1024\n",
            "            Dense_1024\n",
            "          ]\n",
            "          PureAttention_in4_out2\n",
            "          Dense_1024\n",
            "        ]\n",
            "        AssertShape_in2_out2\n",
            "      ]\n",
            "    ]\n",
            "    Add_in2\n",
            "  ]\n",
            "  Select[0,2]_in3_out2\n",
            "  LSTM_1024\n",
            "  LSTM_1024\n",
            "  Dense_33300\n",
            "  LogSoftmax\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkvGVguYIegh"
      },
      "source": [
        "<a name=\"3\"></a>\n",
        "# Part 3:  Training\n",
        "\n",
        "We will now be training our model. Doing supervised training in Trax is pretty straightforward (we learnt it from a short example over [here](https://trax-ml.readthedocs.io/en/latest/notebooks/trax_intro.html#Supervised-training)). We will have to instantiate 3 classes for this: \n",
        "- `TrainTask`,\n",
        "- `EvalTask`, and \n",
        "- `Loop`. \n",
        "We take a closer look at each of these in the code walk-throughs below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XfyRuHnIegh"
      },
      "source": [
        "<a name=\"3.1\"></a>\n",
        "## 3.1  TrainTask\n",
        "\n",
        "The [TrainTask](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.TrainTask) class allows us to define the labeled data to use for training and the feedback mechanisms to compute the loss and update the weights of the neural network. \n",
        "\n",
        "**SETTING UP THE TRAINING TASK:** Below we instantiate a train task. Similarly we instantiate other tasks after that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVWyy4QyIegh"
      },
      "source": [
        "train_task = training.TrainTask(\n",
        "    \n",
        "    # use the train batch stream as labeled data\n",
        "    labeled_data= train_batch_stream,\n",
        "    \n",
        "    # use the cross entropy loss\n",
        "    loss_layer= tl.CrossEntropyLoss(),\n",
        "    \n",
        "    # use the Adam optimizer with learning rate of 0.01\n",
        "    optimizer= trax.optimizers.Adam(.01),\n",
        "    \n",
        "    # use the `trax.lr.warmup_and_rsqrt_decay` as the learning rate schedule\n",
        "    # have 1000 warmup steps with a max value of 0.01\n",
        "    lr_schedule= trax.lr.warmup_and_rsqrt_decay(1000, .01),\n",
        "    \n",
        "    # have a checkpoint every 10 steps\n",
        "    n_steps_per_checkpoint= 10,\n",
        ")"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMnTGvsUIegi"
      },
      "source": [
        "<a name=\"3.2\"></a>\n",
        "## 3.2  EvalTask\n",
        "\n",
        "The [EvalTask](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.EvalTask) allows us to check how the model is performing while it's training. For our application, we want it to continuously report the cross entropy loss and accuracy so that we can get a sanity check every once in a while."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERw8P50gIegi"
      },
      "source": [
        "eval_task = training.EvalTask(\n",
        "    \n",
        "    ## use the eval batch stream as labeled data\n",
        "    labeled_data=eval_batch_stream,\n",
        "    \n",
        "    ## use the cross entropy loss and accuracy as metrics\n",
        "    metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],\n",
        ")"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTNbV6jhIegi"
      },
      "source": [
        "<a name=\"3.3\"></a>\n",
        "## 3.3  Loop\n",
        "\n",
        "The [Loop](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.Loop) class defines the model we will train as well as the train and eval tasks to execute. Its `run()` method allows us to execute the training for a specified number of steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "duArm6EKIegi"
      },
      "source": [
        "# define the output directory\n",
        "output_dir = 'output_dir/'\n",
        "\n",
        "# remove any old model if it exists. restarts training.\n",
        "!rm -f ~/output_dir/model.pkl.gz  \n",
        "\n",
        "# define the training loop\n",
        "training_loop = training.Loop(NMTAttn(mode='train'),\n",
        "                              train_task,\n",
        "                              eval_tasks=[eval_task],\n",
        "                              output_dir=output_dir)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu09ZDW5Iegi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2157f660-ad48-4e65-c9cb-cb33a5b78ac1"
      },
      "source": [
        "# Finally we execute the training loop. This should take around 10-15 minutes in order to complete.\n",
        "training_loop.run(15)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Step      1: Total number of trainable weights: 148492820\n",
            "Step      1: Ran 1 train steps in 174.20 secs\n",
            "Step      1: train CrossEntropyLoss |  10.42580891\n",
            "Step      1: eval  CrossEntropyLoss |  10.40424824\n",
            "Step      1: eval          Accuracy |  0.00000000\n",
            "\n",
            "Step     10: Ran 9 train steps in 477.44 secs\n",
            "Step     10: train CrossEntropyLoss |  10.25261402\n",
            "Step     10: eval  CrossEntropyLoss |  9.95830917\n",
            "Step     10: eval          Accuracy |  0.02429765\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zpxsf2BAIegi"
      },
      "source": [
        "<a name=\"4\"></a>\n",
        "# Part 4:  Testing\n",
        "\n",
        "We will now be using the model you just trained to translate English sentences to German. We will implement this with two functions: \n",
        "- The first allows us to identify the next symbol (i.e. output token) \n",
        "- The second takes care of combining the entire translated string.\n",
        "\n",
        "We will start by first loading in a copy of our model we just coded. It's a fairly bad model. Remember it's a large network to train and the resources at my disposal are pretty shallow. We intend to completely demonstrate the end-to-end process for this translation and all the retrieval steps we have to implement in order to do that.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzKSIq47Iegj"
      },
      "source": [
        "# instantiate the model we built in eval mode\n",
        "model = NMTAttn(mode='eval')\n",
        "\n",
        "# initialize weights from a pre-trained model\n",
        "# model.init_from_file(\"model.pkl.gz\", weights_only=True)\n",
        "model = tl.Accelerate(model)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NM_BnlHoIegj"
      },
      "source": [
        "<a name=\"4.1\"></a>\n",
        "## 4.1  Decoding\n",
        "\n",
        "As discussed in the project report, there are several ways to get the next token when translating a sentence. \n",
        "- For instance, we can just get the most probable token at each step (i.e. Greedy Decoding) or \n",
        "- get a sample from a distribution. \n",
        "\n",
        "We can generalize the implementation of these two approaches by using the `tl.logsoftmax_sample()` method. Let's briefly look at its implementation below:\n",
        "\n",
        "```python\n",
        "def logsoftmax_sample(log_probs, temperature=1.0):  # pylint: disable=invalid-name\n",
        "  \"\"\"Returns a sample from a log-softmax output, with temperature.\n",
        "\n",
        "  Args:\n",
        "    log_probs: Logarithms of probabilities (often coming from LogSofmax)\n",
        "    temperature: For scaling before sampling (1.0 = default, 0.0 = pick argmax)\n",
        "  \"\"\"\n",
        "  # This is equivalent to sampling from a softmax with temperature.\n",
        "  u = np.random.uniform(low=1e-6, high=1.0 - 1e-6, size=log_probs.shape)\n",
        "  g = -np.log(-np.log(u))\n",
        "  return np.argmax(log_probs + g * temperature, axis=-1)\n",
        "```\n",
        "\n",
        "The key things to take away here are: \n",
        "1. it gets random samples with the same shape as your input (i.e. `log_probs`),and \n",
        "2. the amount of \"noise\" added to the input by these random samples is scaled by a `temperature` setting. You'll notice that setting it to `0` will just make the return statement equal to getting the argmax of `log_probs`. \n",
        "\n",
        "\n",
        "**CODING THE FUNCTION WHICH RETURNS THE INDEX OF THE NEXT WORD:** Below we implement the `next_symbol()` function that takes in the `input_tokens` and the `cur_output_tokens`, then return the index of the next word. Some things which didn't seem important before you start coding this:\n",
        "- To get the next power of two, you can compute 2^log_2(token_length + 1). We add 1 to avoid log(0).\n",
        "- From the model diagram in part 2, we know that it takes two inputs. We can feed these with this syntax to get the model outputs: model((input1, input2)). It's up to us to determine which variables below to substitute for input1 and input2. Output also has two elements: [log probabilities, target tokens]. \n",
        "- The log probabilities output will have the shape: (batch size, decoder length, vocab size). It will contain log probabilities for each token in the cur_output_tokens plus 1 for the start symbol introduced by the ShiftRight in the preattention decoder. For example, if cur_output_tokens is [1, 2, 5], the model will output an array of log probabilities each for tokens 0 (start symbol), 1, 2, and 5. To generate the next symbol, we just want to get the log probabilities associated with the last token (i.e. token 5 at index 3). We can slice the model output at [0, 3, :] to get this. It will be up to us to generalize this for any length of cur_output_tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rK1S2lKzIegj"
      },
      "source": [
        "def next_symbol(NMTAttn, input_tokens, cur_output_tokens, temperature):\n",
        "    \"\"\"Returns the index of the next token.\n",
        "\n",
        "    Args:\n",
        "        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.\n",
        "        input_tokens (np.ndarray 1 x n_tokens): tokenized representation of the input sentence\n",
        "        cur_output_tokens (list): tokenized representation of previously translated words\n",
        "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
        "            0.0: same as argmax, always pick the most probable token\n",
        "            1.0: sampling from the distribution (can sometimes say random things)\n",
        "\n",
        "    Returns:\n",
        "        int: index of the next token in the translated sentence\n",
        "        float: log probability of the next symbol\n",
        "    \"\"\"\n",
        "\n",
        "    # set the length of the current output tokens\n",
        "    token_length = len(cur_output_tokens)\n",
        "\n",
        "    # calculate next power of 2 for padding length \n",
        "    padded_length = 2**int(np.ceil(np.log2(token_length + 1))) \n",
        "\n",
        "    # pad cur_output_tokens up to the padded_length\n",
        "    padded = cur_output_tokens + [0] * (padded_length - token_length)\n",
        "    \n",
        "    # model expects the output to have an axis for the batch size in front so\n",
        "    # convert `padded` list to a numpy array with shape (None, <padded_length>) where\n",
        "    # None is a placeholder for the batch size\n",
        "    padded_with_batch = np.expand_dims(padded, axis=0)\n",
        "    \n",
        "    # get the model prediction (remember to use the `NMAttn` argument defined above)\n",
        "    output, _ = NMTAttn((input_tokens, padded_with_batch))\n",
        "\n",
        "    # get log probabilities from the last token output\n",
        "    log_probs = output[0, token_length, :]\n",
        "    \n",
        "    # get the next symbol by getting a logsoftmax sample (*hint: cast to an int)\n",
        "    symbol = int(tl.logsoftmax_sample(log_probs, temperature))\n",
        "\n",
        "    return symbol, float(log_probs[symbol])\n",
        "\n"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgH-qijAIegj"
      },
      "source": [
        "Now we naturally have to implement the `sampling_decode()` function which will call the `next_symbol()` function above several times until the next output is the end-of-sentence token (i.e. `EOS`). It takes in an input string and returns the translated version of that string until the `EOS` token.\n",
        "\n",
        "**IMPLEMENTING SAMPLING DECODE FUNCTION**: Below we implement the `sampling_decode()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LugdMdR1Iegj"
      },
      "source": [
        "def sampling_decode(input_sentence, NMTAttn = None, temperature=0.0, vocab_file=None, vocab_dir=None):\n",
        "    \"\"\"Returns the translated sentence.\n",
        "\n",
        "    Args:\n",
        "        input_sentence (str): sentence to translate.\n",
        "        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.\n",
        "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
        "            0.0: same as argmax, always pick the most probable token\n",
        "            1.0: sampling from the distribution (can sometimes say random things and in our case it will be more horrible)\n",
        "        vocab_file (str): filename of the vocabulary\n",
        "        vocab_dir (str): path to the vocabulary file\n",
        "\n",
        "    Returns:\n",
        "        tuple: (list, str, float)\n",
        "            list of int: tokenized version of the translated sentence\n",
        "            float: log probability of the translated sentence\n",
        "            str: the translated sentence\n",
        "    \"\"\"\n",
        "    \n",
        "    # encode the input sentence\n",
        "    input_tokens = tokenize(input_sentence,vocab_file,vocab_dir)\n",
        "    \n",
        "    # initialize the list of output tokens\n",
        "    cur_output_tokens = []\n",
        "    \n",
        "    # initialize an integer that represents the current output index\n",
        "    cur_output = 0\n",
        "    \n",
        "    # Set the encoding of the \"end of sentence\" token as 1\n",
        "    EOS = 1\n",
        "    \n",
        "    # check that the current output is not the end of sentence token\n",
        "    while cur_output != EOS:\n",
        "        \n",
        "        # update the current output token by getting the index of the next word (hint: use next_symbol)\n",
        "        cur_output, log_prob = next_symbol(NMTAttn, input_tokens, cur_output_tokens, temperature)\n",
        "        \n",
        "        # append the current output token to the list of output tokens\n",
        "        cur_output_tokens.append(cur_output)\n",
        "    \n",
        "    # detokenize the output tokens\n",
        "    sentence = detokenize(cur_output_tokens, vocab_file, vocab_dir)\n",
        "    \n",
        "    return cur_output_tokens, log_prob, sentence"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwt2gZ8aw3lg"
      },
      "source": [
        "### The code below failed to run. We tried looking for solutions on github but haven't been able to resolve it before project submission. Nevertheless, the logic of the code is upright and passes general scrutiny as of the reader too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQBCzbffIegk"
      },
      "source": [
        "# Test the function above. Try varying the temperature setting with values from 0 to 1.\n",
        "# Run it several times with each setting and see how often the output changes.\n",
        "# sampling_decode(\"I love languages.\", model, temperature=0.2, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v37Az4TPIegk"
      },
      "source": [
        "We have set a default value of `0` to the temperature setting in our implementation of `sampling_decode()` above. As one may have noticed in the `logsoftmax_sample()` method, this setting will ultimately result in greedy decoding. \r\n",
        "\r\n",
        "As mentioned in the project report too, this algorithm generates the translation by getting the most probable word at each step. It gets the argmax of the output array of your model and then returns that index. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7EnvW4SIegk"
      },
      "source": [
        "def greedy_decode_test(sentence, NMTAttn=None, vocab_file=None, vocab_dir=None):\n",
        "    \"\"\"Prints the input and output of our NMTAttn model using greedy decode\n",
        "\n",
        "    Args:\n",
        "        sentence (str): a custom string.\n",
        "        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.\n",
        "        vocab_file (str): filename of the vocabulary\n",
        "        vocab_dir (str): path to the vocabulary file\n",
        "\n",
        "    Returns:\n",
        "        str: the translated sentence\n",
        "    \"\"\"\n",
        "    \n",
        "    _,_, translated_sentence = sampling_decode(sentence, NMTAttn, vocab_file=vocab_file, vocab_dir=vocab_dir)\n",
        "    \n",
        "    print(\"English: \", sentence)\n",
        "    print(\"German: \", translated_sentence)\n",
        "    \n",
        "    return translated_sentence"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B88aNZ8gIegl"
      },
      "source": [
        "# # the code below fails to work because of above dependency\n",
        "# your_sentence = 'I love different languages.'\n",
        "\n",
        "# greedy_decode_test(your_sentence, model, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR);"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMQ5NF17Iegl"
      },
      "source": [
        "# greedy_decode_test('You are almost done with the assignment!', model, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR);"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsn5uX2vIegl"
      },
      "source": [
        "<a name=\"4.2\"></a>\n",
        "## 4.2  Minimum Bayes-Risk Decoding\n",
        "\n",
        "As mentioned in the project report, getting the most probable token at each step may not necessarily produce the best results. Another approach is to do Minimum Bayes Risk Decoding or MBR which we also learnt about in the probabilistic retrieval part of our IR course. The general steps to implement this are:\n",
        "\n",
        "1. take several random samples\n",
        "2. score each sample against all other samples\n",
        "3. select the one with the highest score\n",
        "\n",
        "We will be building helper functions for these steps in the following code walk-throughs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xe7bfTFLIegl"
      },
      "source": [
        "<a name='4.2.1'></a>\n",
        "### 4.2.1 Generating samples (Implementing Sample Generation Module)\n",
        "\n",
        "First, let's build a function to generate several samples. You can use the `sampling_decode()` function you developed earlier to do this easily. We want to record the token list and log probability for each sample as these will be needed in the next step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9xVC6M-Iegl"
      },
      "source": [
        "def generate_samples(sentence, n_samples, NMTAttn=None, temperature=0.6, vocab_file=None, vocab_dir=None):\n",
        "    \"\"\"Generates samples using sampling_decode()\n",
        "\n",
        "    Args:\n",
        "        sentence (str): sentence to translate.\n",
        "        n_samples (int): number of samples to generate\n",
        "        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.\n",
        "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
        "            0.0: same as argmax, always pick the most probable token\n",
        "            1.0: sampling from the distribution (can sometimes say random things)\n",
        "        vocab_file (str): filename of the vocabulary\n",
        "        vocab_dir (str): path to the vocabulary file\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (list, list)\n",
        "            list of lists: token list per sample\n",
        "            list of floats: log probability per sample\n",
        "    \"\"\"\n",
        "    # define lists to contain samples and probabilities\n",
        "    samples, log_probs = [], []\n",
        "\n",
        "    # run a for loop to generate n samples\n",
        "    for _ in range(n_samples):\n",
        "        \n",
        "        # get a sample using the sampling_decode() function\n",
        "        sample, logp, _ = sampling_decode(sentence, NMTAttn, temperature, vocab_file=vocab_file, vocab_dir=vocab_dir)\n",
        "        \n",
        "        # append the token list to the samples list\n",
        "        samples.append(sample)\n",
        "        \n",
        "        # append the log probability to the log_probs list\n",
        "        log_probs.append(logp)\n",
        "                \n",
        "    return samples, log_probs"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ey3oWgiIegl",
        "outputId": "90b185ac-cc5f-473b-d90f-f4c83f692dcb"
      },
      "source": [
        "# generate 4 samples with the default temperature (0.6)\n",
        "generate_samples('I love languages.', 4, model, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([[161, 12202, 5112, 3, 1],\n",
              "  [161, 12202, 5112, 3, 1],\n",
              "  [161, 12202, 5112, 3, 1],\n",
              "  [161, 12202, 5112, 3, 1]],\n",
              " [-0.0001735687255859375,\n",
              "  -0.0001735687255859375,\n",
              "  -0.0001735687255859375,\n",
              "  -0.0001735687255859375])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Z_iyYN1Iegm"
      },
      "source": [
        "### 4.2.2 Comparing Overlaps (Implementing Jaccard Index)\n",
        "\n",
        "Let us now build our functions to compare a sample against another. There are several metrics available as shown in the report and one can try experimenting with any one of these. For this assignment, we will be calculating scores for unigram overlaps. One of the more simplest metrics which we also saw in our IR course is the [Jaccard similarity](https://en.wikipedia.org/wiki/Jaccard_index) which gets the intersection over union of two sets. We've implemented it below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgNIUBPxIegm"
      },
      "source": [
        "def jaccard_similarity(candidate, reference):\n",
        "    \"\"\"Returns the Jaccard similarity between two token lists\n",
        "\n",
        "    Args:\n",
        "        candidate (list of int): tokenized version of the candidate translation\n",
        "        reference (list of int): tokenized version of the reference translation\n",
        "\n",
        "    Returns:\n",
        "        float: overlap between the two token lists\n",
        "    \"\"\"\n",
        "    \n",
        "    # convert the lists to a set to get the unique tokens\n",
        "    can_unigram_set, ref_unigram_set = set(candidate), set(reference)  \n",
        "    \n",
        "    # get the set of tokens common to both candidate and reference\n",
        "    joint_elems = can_unigram_set.intersection(ref_unigram_set)\n",
        "    \n",
        "    # get the set of all tokens found in either candidate or reference\n",
        "    all_elems = can_unigram_set.union(ref_unigram_set)\n",
        "    \n",
        "    # divide the number of joint elements by the number of all elements\n",
        "    overlap = len(joint_elems) / len(all_elems)\n",
        "    \n",
        "    return overlap"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPEVP35IIegm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44fb0bd3-6182-42b0-9927-dc2954b8c574"
      },
      "source": [
        "# let's try using the function. remember the result here and compare with the next function below.\n",
        "jaccard_similarity([1, 2, 3], [1, 2, 3, 4])"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.75"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kciKo3K9Iegm"
      },
      "source": [
        "One of the more commonly used metrics in machine translation is the ROUGE score. For unigrams, this is called ROUGE-1 and as we learnt in our IR class, we can output the scores for both precision and recall when comparing two samples. To get the final score, we compute the F1-score as given by:\n",
        "\n",
        "$$score = 2* \\frac{(precision * recall)}{(precision + recall)}$$\n",
        "\n",
        "\n",
        "**ROUGE SIMILARITY FUNCTION**: Below we implement the `rouge1_similarity()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGU5fAUJIegm"
      },
      "source": [
        "# for making a frequency table easily, we use collections module\n",
        "from collections import Counter\n",
        "\n",
        "def rouge1_similarity(system, reference):\n",
        "    \"\"\"Returns the ROUGE-1 score between two token lists\n",
        "\n",
        "    Args:\n",
        "        system (list of int): tokenized version of the system translation\n",
        "        reference (list of int): tokenized version of the reference translation\n",
        "\n",
        "    Returns:\n",
        "        float: overlap between the two token lists\n",
        "    \"\"\"    \n",
        "    \n",
        "    # make a frequency table of the system tokens (using the Counter class)\n",
        "    sys_counter = Counter(system)\n",
        "\n",
        "    # make a frequency table of the reference tokens (using the Counter class)\n",
        "    ref_counter = Counter(reference)\n",
        "    \n",
        "    # initialize overlap to 0\n",
        "    overlap = 0\n",
        "    \n",
        "    # run a for loop over the sys_counter object (can be treated as a dictionary)\n",
        "    for token in sys_counter:\n",
        "        \n",
        "        # lookup the value of the token in the sys_counter dictionary (using the get() method)\n",
        "        token_count_sys = sys_counter.get(token,0)\n",
        "        \n",
        "        # lookup the value of the token in the ref_counter dictionary (using the get() method)\n",
        "        token_count_ref = ref_counter.get(token,0)\n",
        "        \n",
        "        # update the overlap by getting the smaller number between the two token counts above\n",
        "        overlap += min(token_count_sys, token_count_ref)\n",
        "    \n",
        "    # get the precision (i.e. number of overlapping tokens / number of system tokens)\n",
        "    \n",
        "    precision = overlap / sum(sys_counter.values())\n",
        "    \n",
        "    # get the recall (i.e. number of overlapping tokens / number of reference tokens)\n",
        "\n",
        "    recall = overlap / sum(ref_counter.values())\n",
        "    \n",
        "    if precision + recall != 0:\n",
        "        # compute the f1-score\n",
        "        rouge1_score = 2 * ((precision * recall)/(precision + recall))\n",
        "    else:\n",
        "        rouge1_score = 0 \n",
        "    \n",
        "    return rouge1_score"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx_1mFvCIegn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a7898fc-b42e-499d-d90f-353f1a194a85"
      },
      "source": [
        "# notice that this produces a different value from the jaccard similarity earlier\n",
        "rouge1_similarity([1, 2, 3], [1, 2, 3, 4])"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8571428571428571"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdQ_SB6HIegn"
      },
      "source": [
        "### 4.2.3 Overall score\n",
        "\n",
        "We will now build a function to generate the overall score for a particular sample. As mentioned earlier, we need to compare each sample with all other samples. For instance, if we generated 30 sentences, we will need to compare sentence 1 to sentences 2 to 30. Then, we compare sentence 2 to sentences 1 and 3 to 30, and so forth. At each step, we get the average score of all comparisons to get the overall score for a particular sample. To illustrate, these will be the steps to generate the scores of a 4-sample list.\n",
        "\n",
        "1. Get similarity score between sample 1 and sample 2\n",
        "2. Get similarity score between sample 1 and sample 3\n",
        "3. Get similarity score between sample 1 and sample 4\n",
        "4. Get average score of the first 3 steps. This will be the overall score of sample 1.\n",
        "5. Iterate and repeat until samples 1 to 4 have overall scores.\n",
        "\n",
        "We will be storing the results in a dictionary/hashmap for easy $O(1)$ lookups.\n",
        "\n",
        "\n",
        "**COMPUTING THE AVERAGE OVERLAP FUNCTION**: Below we implement the `average_overlap()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQTp0CGMIegn"
      },
      "source": [
        "def average_overlap(similarity_fn, samples, *ignore_params):\n",
        "    \"\"\"Returns the arithmetic mean of each candidate sentence in the samples\n",
        "\n",
        "    Args:\n",
        "        similarity_fn (function): similarity function used to compute the overlap\n",
        "        samples (list of lists): tokenized version of the translated sentences\n",
        "        *ignore_params: additional parameters will be ignored\n",
        "\n",
        "    Returns:\n",
        "        dict: scores of each sample\n",
        "            key: index of the sample\n",
        "            value: score of the sample\n",
        "    \"\"\"  \n",
        "    \n",
        "    # initialize dictionary/hashmap\n",
        "    scores = {}\n",
        "    \n",
        "    # run a for loop for each sample\n",
        "    for index_candidate, candidate in enumerate(samples):    \n",
        "      \n",
        "      # initialize overlap to 0.0\n",
        "      overlap = 0.0\n",
        "\n",
        "      \n",
        "      # run a for loop for each sample\n",
        "      for index_sample, sample in enumerate(samples): \n",
        "\n",
        "          # skip if the candidate index is the same as the sample index\n",
        "          if index_candidate == index_sample:\n",
        "            continue\n",
        "              \n",
        "          # get the overlap between candidate and sample using the similarity function\n",
        "          sample_overlap = similarity_fn(candidate,sample)\n",
        "\n",
        "          # add the sample overlap to the total overlap\n",
        "          overlap += sample_overlap\n",
        "\n",
        "      # get the score for the candidate by computing the average\n",
        "      score = overlap/index_sample\n",
        "      \n",
        "      # save the score in the dictionary. use index as the key.\n",
        "      scores[index_candidate] = score\n",
        "\n",
        "    return scores"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfnDoE0SIegn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3b5ceb0-c87f-42f8-c2c9-3aa952760acd"
      },
      "source": [
        "average_overlap(jaccard_similarity, [[1, 2, 3], [1, 2, 4], [1, 2, 4, 5]], [0.4, 0.2, 0.5])"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 0.45, 1: 0.625, 2: 0.575}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnNqvOssIego"
      },
      "source": [
        "In practice, we also found it common to see the weighted mean being used to calculate the overall score instead of just the arithmetic mean. We have implemented it below but we haven't run experiements to see which one gives us better results. Anyway because our sampling decode function fails to run, we couldn't proceed further with our analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1cSEUWwIego"
      },
      "source": [
        "def weighted_avg_overlap(similarity_fn, samples, log_probs):\n",
        "    \"\"\"Returns the weighted mean of each candidate sentence in the samples\n",
        "\n",
        "    Args:\n",
        "        samples (list of lists): tokenized version of the translated sentences\n",
        "        log_probs (list of float): log probability of the translated sentences\n",
        "\n",
        "    Returns:\n",
        "        dict: scores of each sample\n",
        "            key: index of the sample\n",
        "            value: score of the sample\n",
        "    \"\"\"\n",
        "    \n",
        "    # initialize dictionary\n",
        "    scores = {}\n",
        "    \n",
        "    # run a for loop for each sample\n",
        "    for index_candidate, candidate in enumerate(samples):    \n",
        "        \n",
        "        # initialize overlap and weighted sum\n",
        "        overlap, weight_sum = 0.0, 0.0\n",
        "        \n",
        "        # run a for loop for each sample\n",
        "        for index_sample, (sample, logp) in enumerate(zip(samples, log_probs)):\n",
        "\n",
        "            # skip if the candidate index is the same as the sample index            \n",
        "            if index_candidate == index_sample:\n",
        "                continue\n",
        "                \n",
        "            # convert log probability to linear scale\n",
        "            sample_p = float(np.exp(logp))\n",
        "\n",
        "            # update the weighted sum\n",
        "            weight_sum += sample_p\n",
        "\n",
        "            # get the unigram overlap between candidate and sample\n",
        "            sample_overlap = similarity_fn(candidate, sample)\n",
        "            \n",
        "            # update the overlap\n",
        "            overlap += sample_p * sample_overlap\n",
        "            \n",
        "        # get the score for the candidate\n",
        "        score = overlap / weight_sum\n",
        "        \n",
        "        # save the score in the dictionary. use index as the key.\n",
        "        scores[index_candidate] = score\n",
        "    \n",
        "    return scores"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMalAJRxIego",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffb87558-6da5-4d7a-8889-ee7aebe2c5b0"
      },
      "source": [
        "weighted_avg_overlap(jaccard_similarity, [[1, 2, 3], [1, 2, 4], [1, 2, 4, 5]], [0.4, 0.2, 0.5])"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 0.44255574831883415, 1: 0.631244796869735, 2: 0.5575581009406329}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "po7F5KwLIego"
      },
      "source": [
        "### 4.2.4 Putting it all together to get a translated sentence\n",
        "\n",
        "We will now put everything together and develop the Minimum Bayes Risk decoding `mbr_decode()` function. We uses the helper functions we developed above to complete this. We need to do 3 things in order to complete this:\n",
        "- We have to generate samples, \n",
        "- get the score for each sample, \n",
        "- get the highest score among all samples, \n",
        "- then detokenize this sample to get the translated sentence.\n",
        "\n",
        "\n",
        "**FINAL IMPLEMENTATION OF MINIMUM BAYES RISK MODEL**: Below we implement the `mbr_overlap()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PC9-rpBGIegp"
      },
      "source": [
        "def mbr_decode(sentence, n_samples, score_fn, similarity_fn, NMTAttn=None, temperature=0.6, vocab_file=None, vocab_dir=None):\n",
        "    \"\"\"Returns the translated sentence using Minimum Bayes Risk decoding\n",
        "\n",
        "    Args:\n",
        "        sentence (str): sentence to translate.\n",
        "        n_samples (int): number of samples to generate\n",
        "        score_fn (function): function that generates the score for each sample\n",
        "        similarity_fn (function): function used to compute the overlap between a pair of samples\n",
        "        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.\n",
        "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
        "            0.0: same as argmax, always pick the most probable token\n",
        "            1.0: sampling from the distribution (can sometimes say random things)\n",
        "        vocab_file (str): filename of the vocabulary\n",
        "        vocab_dir (str): path to the vocabulary file\n",
        "\n",
        "    Returns:\n",
        "        str: the translated sentence\n",
        "    \"\"\"\n",
        "    \n",
        "    # we start generating samples\n",
        "    samples, log_probs = generate_samples(sentence, n_samples, NMTAttn, temperature, vocab_file, vocab_dir)\n",
        "    \n",
        "    # use the scoring function to get a dictionary of scores\n",
        "    # we pass in the relevant parameters as shown in the function definition of \n",
        "    # the mean methods we developed earlier\n",
        "    scores = score_fn(similarity_fn, samples, log_probs)\n",
        "\n",
        "    # find the key with the highest score\n",
        "    max_index = max(scores, key=scores.get)\n",
        "    \n",
        "    # detokenize the token list associated with the max_index\n",
        "    translated_sentence = detokenize(samples[max_index], vocab_file, vocab_dir)\n",
        "    \n",
        "    return (translated_sentence, max_index, scores)"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMbteQjJIegp"
      },
      "source": [
        "TEMPERATURE = 1.0\n",
        "\n",
        "# put a custom string here\n",
        "your_sentence = 'She speaks English and German.'"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "El9UIeq-Iegp"
      },
      "source": [
        "# mbr_decode(your_sentence, 4, weighted_avg_overlap, jaccard_similarity, model, TEMPERATURE, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)[0]"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVEwfYJsIegp"
      },
      "source": [
        "# mbr_decode('DANGER AHEAD!', 4, average_overlap, rouge1_similarity, model, TEMPERATURE, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)[0]"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59D-MajYIegp"
      },
      "source": [
        "# mbr_decode('PROJECT COMPLETED!', 4, average_overlap, rouge1_similarity, model, TEMPERATURE, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)[0]"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pybADrVh4Avv"
      },
      "source": [
        "## Code / Data / Images References:\r\n",
        "- https://www.coursera.org/learn/attention-models-in-nlp\r\n",
        "- http://opus.nlpl.eu/EMEA.php\r\n",
        "- [Trax Tutorials](https://trax-ml.readthedocs.io/en/latest/)\r\n",
        "- [Develop a Neural Machine Translation System from Scratch](https://machinelearningmastery.com/develop-neural-machine-translation-system-keras/)\r\n",
        "- [Neural Machine Translation](https://google.github.io/seq2seq/nmt/)\r\n",
        "- [Python for NLP: Neural Machine Translation with Seq2Seq in Keras](https://stackabuse.com/python-for-nlp-neural-machine-translation-with-seq2seq-in-keras/)\r\n",
        "\r\n"
      ]
    }
  ]
}